\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Proper Time Series}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=PTS}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(scales)
library(tidyverse)
library(lubridate)
library(broom)
library(funModeling)
library(forecast)
library(gridExtra)
library(writexl)
library(plotly)
library(ggfortify)
library(latex2exp)
library(tseries)
library(readr)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions
<<echo=FALSE>>=
TRJ.FFT <- function(signal.df) {
    # This function calculates the FFT for a time-series stored in a data frame with as first 
    # column the time (or order of measurement) and as second column the vector of measurements.
    # The result is a list. 
    # The first element of the list is freqspec: the N frequencies plus for each frequency the 
    # amplitude and phase.
    # The second element of the list is resultaat: a data frame with those frequencies for which 
    # the amplitude  is at least 33% of the maximum amplitude. 
    # The data frame is sorted from highes amplitude to lowest. 
    # This data frame can be seen as containing the most influential frequencies.
    signal <- signal.df
    names(signal) <- c("t","x")
    N <- nrow(signal)
    Ts <- as.numeric(signal$t[2]-signal$t[1])
    Fs <- 1/Ts
    # Calculation of the double sided en single sided spectrum
    z <- fft(signal$x)
    P2 <- Mod(z/N)
    P1 <- P2[1:((N/2)+1)]
    P1[2:(length(P1)-1)] <- 2*P1[2:(length(P1)-1)]
    freq <- seq(0, (Fs/2)-(Fs/N), Fs/N)
    freqspec <- data.frame(freq=freq,amp=P1[1:(N/2)],arg=Arg(z[1:(N/2)]))
    # Finding the most important elements in the frequency spectrum
    grens <- ifelse(freqspec$freq[freqspec$amp==max(freqspec$amp)]==0,max(freqspec$amp[2:nrow(freqspec)])/3,max(freqspec$amp)/3)
    aantal <- length(freqspec$amp[freqspec$amp>grens])
    resultaat <- data.frame(freq=rep(0,aantal), amp=rep(0,aantal), fasehoek=rep(0,aantal))
    resultaat <- data.frame(freq=freqspec$freq[freqspec$amp>grens],
                            amp=freqspec$amp[freqspec$amp>grens],
                            fasehoek_pi=freqspec$arg[freqspec$amp>grens]/pi)
    resultaat <- resultaat[order(-resultaat$amp),]
    return(list("freqspec"=freqspec,"resultaat"=resultaat))
}
@


\frontmatter
\chapter*{Proper Time Series}

\mainmatter

\chapter{Introduction}

\section{Time series}\index{time series}

A \emph{time series} is a sequence of values of a quantity $y$ taken at consecutive points in time, in a  strict order. Often the points in time are absolute: Monday March 18th 2019, Tuesday March 19th 2019, etc. The interval between these points in time is usually constant. It can be nanoseconds or millenia. With each point in time we can associate a time index\index{time index} going from $1$ for the first point in time, to $n$ for the last point in time of our data series.

<<label=startgrafiek, fig=TRUE, include=FALSE, echo=FALSE>>=
# Number of elements in the data set
  N <- 1000
# Random number generator seed
  set.seed(2019)
# error parameters
  mu <- 0
  sigma <- 3
# trend model parameters
  b0 <- 0
  b1 <- 0.01
  b2 <- 0.00005
# seasonality 1
  amp1 <- 3*sigma
  teta1 <- 0
  T1 <- N/9
# seasonality 2
  amp2 <- sigma
  teta2 <- pi/3
  T2 <- N/39
# constant for combined additive/multiplicative time-series
  c <- 0.05
#
# setting up the different components
#
  construct <- data.frame(t = seq(1:N),
                            error = rnorm(N, mean = mu, sd = sigma))
  construct %>% mutate(trend = (b0 + b1*t +b2*t^2)) -> construct
  construct %>% mutate(season1 = amp1*sin(2*pi*t/T1 + teta1)) -> construct
  construct %>% mutate(season2 = amp2*sin(2*pi*t/T2 + teta2)) -> construct
#
# contructing the additive time series
construct %>% mutate(add = error + trend + season1 + season2) -> construct
ggplot(data = construct, aes(x = t)) +
    geom_line(aes(y = add), size = 0.35, color="red") +
    scale_y_continuous(limits= c(-25, 75)) +
    labs(y="y") +
    JT.theme
@
  
\begin{marginfigure}[-2cm]
\includegraphics[width=1\textwidth]{PTS-startgrafiek}
\caption{}
\label{fig:startgrafiek}
\setfloatalignment{b}
\end{marginfigure}

\newthought{Examples}
\medskip
\begin{itemize}
	\item the temperature of a blast furnace taken every second
	\item the opening share price of a company on Nasdaq (daily)
	\item the number of federal prisoners in the USA (monthly)
  \item the number of bicycles crossing an intersection within a period of one hour
	\item the amount of rainfall in a specific place within a period of one month
\end{itemize}

The quantity $y$ that we measure at index point $t$, we denote by $y_{t}$. Usually $y_{t}$ will depend in some measure on chance: Boeing's share price can suddenly plummet when one of its airplanes has an accident, a temperature measurement can be somewhat lower because someone left open a door, ... Therefore we consider the measurement $y$ to be one of many possible values of a \emph{chance variable} $Y$. This is true for the value $y_{t}$ at every point in time $t$: it is the value taken by the chance variable $Y_{t}$ at time $t$.

\begin{marginfigure}[-5cm]
\includegraphics[width=1\textwidth]{"PTS-startgrafiekchance"}
\caption{}
\label{fig:startgrafiekchance}
\setfloatalignment{b}
\end{marginfigure}

In Figure~\ref{fig:startgrafiekchance} we can see that the red graph is only one of many (infinite) possible graphs. We have included 4 alternatives (grey), and a detail for $t=500$ shows some representations of $Y_{500}$.


When we treat $Y_{t}$ as a chance variable, it follows that it has a probability distribution, characterised by its parameters such as the mean ($\mu_{t}$), the standard deviation ($\sigma_{t}$), etc. These parameters can change so that $\mu_{1} \neq \mu_{2}$ and $\sigma_{1} \neq \sigma_{2}$. In general:
\begin{equation}
	\mu_{t}=\mu_{t}(t) \quad and \quad \sigma_{t}=\sigma_{t}(t)
\end{equation}

\section{The time series as a sample}

For most time series we do not have access to all possible values that $Y_{t}$ can take. We usually have only one value $y_{t}$ at each time point $t$. As an estimate of the mean at time point $t$ we can use the observed value $y_{t}$, but this is a very crude estimation. 

For some time series we could have the (visual) impression that our observations $y_{t}$ seem to fluctuate around a constant value. In these conditions we can make the assumption that the mean does not change with $t$. A time series with this behaviour is called \emph{stationary in the mean}\index{stationary!in the mean}, and we can estimate this constant value of $\mu$ from the sample mean:
\begin{equation}
	\bar{y} = \frac{1}{n}\sum_{i=1}^{i=n}y_{i}
\end{equation}

The same argument can be used for the \emph{variance} of $Y_{t}$. In some time series we can simplify the situation by assuming that the population variance $\sigma^{2}$ is a constant, and that we can estimate it from the sample variance:
\begin{equation}
	var(y) = \frac{1}{n-1}\sum_{i=1}^{i=n-1}\left( y_{i} - \bar{y} \right)^{2}
\end{equation}

A time series where the variance is a constant is called \emph{stationary in the variance}\index{stationary in the variance}\index{stationary!in the variance}.

A time series that is \emph{stationary in the mean} and \emph{stationary in the variance} is called \emph{second order stationary}\index{second order stationarity}\index{stationary!second order}. The idea of \emph{stationarity} will play an important role when we want to use the time series to make a forecast.

\section{The trouble with forecasting}

Forecasting\index{forecast}, by definition, is about the future: you are stepping into unknown territory and this brings with it Niels Bohr's warning:
\begin{quotation}
 ''It is very hard to predict, especially the future"
\end{quotation}

Because we, humans, have the privilege of vision, we have the ability to see patterns. In Figure~\ref{fig:Microsoftextra}-left we can see that the observations of $y$ lie approximately within the blue region.  We also have an inbuilt tendency to extrapolate. Knowing only the data given in Figure~\ref{fig:Microsoftextra}-left we are tempted to make assumptions for what lies in the future and thus, to make a forecast (Figure~\ref{fig:Microsoftextra}-middle). Those who are very convinced of their predictive powers, could even imagine a pattern in the wiggly bits of the data, and come up with an even more detailed forecast (Figure~\ref{fig:Microsoftextra}-right).

\begin{figure*}
\includegraphics[width=1\textwidth]{"Graphics/Microsoftextra"}
\caption{Our predictive powers}
\label{fig:Microsoftextra}
\setfloatalignment{b}
\end{figure*}

Sadly, we would have been completely wrong. The actual evolution of this particular time series was completely different from what we expected (Figure~\ref{fig:Microsoft5}).

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{"Graphics/Microsoft5"}
\caption{}
\label{fig:Microsoft5}
\setfloatalignment{b}
\end{marginfigure}

Figure~\ref{fig:Microsoftextra} shows the time series of the value of Microsoft shares from 01-01-1998 up to 31-03-2000. Then, in April 2000, share prices dropped 15.6 percent (Figure~\ref{fig:Microsoft5}) when it announced disappointing earnings and took a massive \$900 million writedown due to unsold copies of its mobile operating system ''Surface RT". Forecasting of share prices based on their time series is notoriously difficult, and the example should warn us that our ''intuition" is not a good guide for forecasting.

\section{The ideal time series for forecasting}\index{stationary}\index{time series!stationary}
\label{sec:ideal}
What should a time series look like so that we can make confident forecasts? What should the values from the past look like so that they can \emph{safely be extended into the future}? They should be ''the same", which means they have the same underlying structure. This structure can be represented by statistical properties:
\begin{itemize}
	\item the time series should be \emph{stationary in the mean}. 
	\item the time series should be \emph{stationary in the variance}.
	\item or more general: the time series should be \emph{second order stationary}
\end{itemize}

We can easily generate a stationary time series: if the sequential observations $y_{t}$ are \emph{Independent}\index{independence} and come from an \emph{Identical}\index{identical} \emph{probability Distribution} (abbreviated as \emph{IID}) \index{IID = Independent Identical Distribution} the result will be a stationary time series. For example we can simulate the throw of a dice: we use the \textit{sample}-function from \textbf{\textsf{R}} and, by replacing each drawn number, we make sure that each throw is independent (Figure~\ref{fig:stationarydice1}).

<<label=stationarydice1, fig=TRUE, include=FALSE, echo=FALSE>>=
n <- 100 # number of time points
N <- 100 # number of time series
stationary.dice <- matrix(sample(c(1:6), n*N, replace = TRUE), nrow = n, ncol = N)
stationary.norm <- matrix(rnorm(n*N, 3.5, 1), nrow = n, ncol = N)
stationary <- data.frame(t = seq(1,N), 
                              ydice1 = stationary.dice[ ,1], 
                              ydice2 = stationary.dice[ ,2], 
                              ydice3 = stationary.dice[ ,3], 
                              ydice4 = stationary.dice[ ,4], 
                              ydice5 = stationary.dice[ ,5],
                              ynorm1 = stationary.norm[ ,1],
                              ynorm2 = stationary.norm[ ,2],
                              ynorm3 = stationary.norm[ ,3],
                              ynorm4 = stationary.norm[ ,4],
                              ynorm5 = stationary.norm[ ,5],
                         mean.dice = apply(stationary.dice, 1, FUN = mean),
                         sd.dice = apply(stationary.dice, 1, FUN = sd),
                         mean.norm = apply(stationary.norm, 1, FUN = mean),
                         sd.norm = apply(stationary.norm, 1, FUN = sd))
ggplot(data = stationary, aes(x = t)) +
  geom_line(aes(y = ydice1), col = "red") +
  scale_y_continuous(limits=c(0, 7), breaks = seq(0:7)) +
  labs(title = "Example of stationary time series\n(uniform distribution)", x = "t", y = "result") +
  JT.theme
@

\begin{marginfigure}[-12cm]
\includegraphics[width=1\textwidth]{PTS-stationarydice1}
\caption{Stationary time series: $y_{t}$}
\label{fig:stationarydice1}
\setfloatalignment{b}
\end{marginfigure}

The advantage of constructing your own time series, is that you can redo it! As an example we construct 100 time series, each with 100 time points. Figure~\ref{fig:stationarydice2} shows 5 of these 100 time series. This gives us an idea of the chance variable $Y_{t}$.

<<label=stationarydice2, fig=TRUE, include=FALSE, echo=FALSE>>=
color <- c('chartreuse3', 'cornflowerblue', 'darkgoldenrod1', 'peachpuff3','mediumorchid2')
ggplot(data = stationary, aes(x = t)) +
  geom_line(aes(y = ydice1), col = color[1]) +
  geom_line(aes(y = ydice2), col = color[2]) +
  geom_line(aes(y = ydice3), col = color[3]) +
  geom_line(aes(y = ydice4), col = color[4]) +
  geom_line(aes(y = ydice5), col = color[5]) +
  scale_y_continuous(limits=c(0, 7), breaks = seq(0:6)) +
  labs(title = "Example of stationary time series\n(uniform distribution)", x = "t", y = "result") +
  JT.theme
@

\begin{marginfigure}[-5cm]
\includegraphics[width=1\textwidth]{PTS-stationarydice2}
\caption{Stationary time series: $Y_{t}$}
\label{fig:stationarydice2}
\setfloatalignment{b}
\end{marginfigure}

These 5 time series are clearly not identical. For example: at time point $t=50$ we have the following values for $y_{50}$:
<<>>=
stationary[50, 2:6]
@

But when we calculate the mean value (red) and the standard deviation (blue) of $y_{t}$ at each time point $t$ for all 100 time series we get Figure~\ref{fig:stationarydice3}:

<<label=stationarydice3, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = stationary, aes(x = t)) +
  geom_line(aes(y = ydice1), col = color[1]) +
  geom_line(aes(y = ydice2), col = color[2]) +
  geom_line(aes(y = ydice3), col = color[3]) +
  geom_line(aes(y = ydice4), col = color[4]) +
  geom_line(aes(y = ydice5), col = color[5]) +
  geom_line(aes(y = mean.dice), col = "red", size = 1.5) +
  geom_line(aes(y = sd.dice), col = "blue", size = 1.5) +
  scale_y_continuous(limits=c(0, 7), breaks = seq(1:6)) +
  labs(title = "Example of stationary time series\n(uniform distribution)", x = "t", y = "result") +
  JT.theme
@

\begin{marginfigure}[-3cm]
\includegraphics[width=1\textwidth]{PTS-stationarydice3}
\caption{Stationary time series: $Y_{t}$ with mean and standard deviation}
\label{fig:stationarydice3}
\setfloatalignment{b}
\end{marginfigure}

The mean and the standard deviation are almost exactly constant and equal to the expected value (3.5 for the mean and 1.44 for the standard deviation)\sidenote{This is of course what we expect based on the theory of sampling distributions}. This time series is \emph{second order stationary}.

Another example: we construct a stationary time series by taking, at each point in time $t$, a random draw from a normal distribution (in this example N($\mu=3.5$, $\sigma=1$). Again, the mean and standard deviation of the 100 simulated time series calculated at each time point $t$, are not exactly constant, but very close to the mean (3.5) and the standard deviation (1) of the normal distribution that we used. This time series is \emph{second order stationary}.

<<label=stationarynorm, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = stationary, aes(x = t)) +
  geom_line(aes(y = ynorm1), col = color[1]) +
  geom_line(aes(y = ynorm2), col = color[2]) +
  geom_line(aes(y = ynorm3), col = color[3]) +
  geom_line(aes(y = ynorm4), col = color[4]) +
  geom_line(aes(y = ynorm5), col = color[5]) +
  geom_line(aes(y = mean.norm), col = "red", size = 1.5) +
  geom_line(aes(y = sd.norm), col = "blue", size = 1.5) +
  scale_y_continuous(limits=c(0, 7), breaks = seq(0:6)) +
  labs(title = "Example of stationary time series\n(normal distribution)", x = "t", y = "result") +
  JT.theme
@

\begin{marginfigure}[-1cm]
\includegraphics[width=1\textwidth]{PTS-stationarynorm}
\caption{Stationary time series: $Y_{t}$ with mean and standard deviation}
\label{fig:stationarynorm}
\setfloatalignment{b}
\end{marginfigure}

Figure~\ref{fig:stationarydice1} and Figure~\ref{fig:stationarynorm} (which is Gaussian noise\index{noise}\index{Gaussian noise}\index{noise!Gaussian} have a very irregular look and it might come as a surprise that we consider these \emph{stationary} time series excellent material for forecasting! From a statistical point of view however it simply indicates that when we accept that chance influences the values $y_{t}$ of our time series, we also have to accept that we can only provide a confidence interval for the future values $y_{n+1}, y_{n+2}$. If we have reason to believe that $Y_{t}$ is normally distributed we can define a $\alpha$ confidence interval for the first forecasted value $y_{n+1}$\sidenote{We assume here that the variance will remain constant. For a one-step forecast this may be acceptable, but for longer forecasts we will have to increase the variance}: 
\begin{equation}
	\bar{y} - ks'_{y} \leq y_{n+1} \leq \bar{y} - ks'_{y} \quad with \quad k=tinv \left( 1-\frac{\alpha}{2} \right)
\end{equation}

For a 95\% confidence interval $k \sim 2$.

If $Y_{t}$ is not normally distributed, than we can fall back on Chebyshev's inequality to obtain a confidence interval. For a 95\%-interval $k \sim 4.5$.

\section{Is a given time series stationary?}

In \ref{sec:ideal} we constructed two stationary time series using the \emph{IID-principle}\index{IID-principle}: \emph{Independent}\index{independence} samples taken from an \emph{Identical} \emph{Distribution}. 

In reality we have to work with time series that are presented to us. A first sign of possible \emph{stationarity} would be that the elements of this time series are \emph{independent} of one another. Conversely: if we detect that the elements of this time series are \emph{not independent}, it would make it unlikely that this given time series would be \emph{stationary}. \emph{Independence} can be checked using the concepts of \emph{covariance} and \emph{correlation}. 

\subsection{Covariance and Correlation}\index{covariance}\index{correlation}
In general, when we have two random variables $A$ and $B$ the \emph{covariance} of these two is given by:
\begin{equation}
	cov(A,B)= E\left[ (A - \mu_{A})(B - \mu_{B}) \right] 
\end{equation}

Most often we do not have access to all possible values of $A$ and $B$, but we only have a set of $n$ values ($a_{1} \ldots a_{n}$) and $n$ values ($b_{1} \ldots b_{n}$) (a sample from $A$ and a sample from $B$). These samples form $n$ pairs $(a_{i}, b_{i}) \quad i=1 \ldots n$. We can estimate the population means $\mu_{A}$ and $\mu_{B}$ from
\begin{equation}
	\bar{a} = \frac{1}{n}\sum_{i=1}^{i=n}a_{i} \quad and \quad \bar{b} = \frac{1}{n}\sum_{i=1}^{i=n}b_{i}
\end{equation}

and calculate the \emph{sample covariance}\index{sample covariance}\index{covariance!of sample}:
\begin{equation}
	Cov(a,b)= \frac{1}{n-1}\sum_{i=1}^{i=n}(a_{i}-\bar{a})(b_{i}-\bar{b})
\end{equation}

From this definition we can see that if we calculate the sample covariance of n pairs $(a_{i}, b_{i})$ and of n pairs $(c_{i}, d_{i})$, the result will be larger, in absolute value, for the pairs which have a components with a lot of variance. Therefore it is better to use a dimensionless measure by normalising, the \emph{sample correlation}
\begin{equation}
	Cor(a,b)=\frac{Cov(a,b)}{sd(a)sd(b)}
\end{equation}

where
\begin{equation}
	sd(a)=\sqrt{var(a)} \quad and \quad sd(b)=\sqrt{var(b)}
\end{equation}

$Cor(a,b)$ is dimensionless and its value will lie between -1 and 1. High absolute values of $Cor(a,b)$ indicate a strong correlation, values around 0 indicate no correlation at all.

\subsection{Using covariance and correlation in time series: autocorrelation}
\label{subsec:autocor}

In time series we usually have only one set of $n$ values $y_{t}$, one for each time point $t$. Suppose the values of this times series were given by a \emph{global} formula
\begin{equation}
	y_{t} = 5 + 0.2t
\end{equation}

For time points $t=1 \ldots 5$ the corresponding values of $y$ would be
\begin{equation}
	5.2, \quad 5.4, \quad 5.6, \quad 5.8, \quad 6.0
\end{equation}

Another way to construct this time series would be:
\begin{equation}
	y_{t}=y_{t-1} + 0.2 \quad with \quad y_{1}=5.2
\end{equation}
This is a \emph{local} definition\sidenote{linear recursion equation. See "Linear Recurrence Relations with Constant Coefficients", P. Levrie, University of Antwerp}\index{linear recursion equation} that tells us that the value $y_{t}$ at time point $t$ is \emph{related} to value at the previous time point. The time series is thus \textbf{co}-related \emph{with itself}. This type of correlation is called \emph{autocorrelation}\index{autocorrelation}.

If we want to calculate \emph{covariance} and \emph{correlation} we need pairs $(a_{i}, b_{i})$. Comparing each point with its previous point we get $(n-1)=4$ pairs: 
\begin{equation}
	(y_{2},y_{1}), \quad (y_{3},y_{2}), \quad (y_{4},y_{3}), \quad (y_{5},y_{4})
\end{equation}

This we call \emph{autocorrelation Lag1}. We can also compare each point with the data point two time points before. We get $(n-2)=3$ pairs (\emph{autocorrelation Lag2}): 
\begin{equation}
	(y_{3},y_{1}), \quad (y_{4},y_{2}), \quad (y_{5},y_{3})
\end{equation}

The \emph{sample autocovariance function Lagk} is defined by\sidenote[][-1cm]{for all lags we still devide by the total length $n$ of the time series, and not by the number of pairs $(n-k)$ that are used for comparison. We also use everywhere $\bar{y}$, the average of the complete time series, and not the average of the partial time series. If the time series is long ($n$ large), this does not influence the result greatly. For small values of $n$ the effect can be substantial.}:
\begin{equation}
	c_{k}=\frac{1}{n}\sum_{i=1}^{i=n-k}(y_{i+k} - \bar{y})(y_{i} - \bar{y})
	\label{eq:autocovariance}
\end{equation}

and the \emph{sample autocorrelation function Lagk} is defined by:
\begin{equation}
	r_{k}=\frac{c_{k}}{c_{0}}
\end{equation}

From this definition it is clear that $r_{0}=1$, which is what we expected: every time series is perfectly correlated with itself at Lag0. The \textbf{\textsf{R}}-function \textit{acf} gives us a graph of the autocorrelation within a time series for different lags. For our example where $y_{t}=5+0.2t$ ($n=1000$) we see in Figure~\ref{fig:autocorlin} perfect autocorrelation for all lags. The blue lines indicate the confidence interval for the hypothesis that lagk is not autocorrelated\sidenote[][+4cm]{CIhigh = qnorm(0.975)/sqrt(n)}.

<<label=autocorlin, fig=TRUE, include=FALSE, echo=FALSE>>=
n <- 1000
autocorlin <- data.frame(t = seq(1,n), y = 0)
autocorlin$y <- 5 + 0.2*autocorlin$t
autolin <- data.frame(lag = acf(autocorlin$y, lag.max=10, plot = FALSE)$lag, 
                   acf = acf(autocorlin$y, lag.max=10, plot = FALSE)$acf)
ci.high <- qnorm(0.975)/sqrt(n)
ci.low <- - ci.high
ggplot(data = autolin, aes(x = lag, y = acf)) +
        geom_segment(aes(xend = lag, yend = 0), alpha = 0.4, size = 2) +
        geom_hline(yintercept = 0) +
        geom_hline(yintercept = ci.high, linetype = "dashed", col = "blue") +
        geom_hline(yintercept = ci.low, linetype = "dashed", col = "blue") +
        scale_x_continuous(limits=c(0, 10), breaks = seq(0,10,1)) +
        scale_y_continuous(limits=c(-1, 1), breaks = seq(-1,1,0.1)) +
        labs(title = "Autocorrelation (linear dependency)", x = "lag", y = "result") +
        JT.theme
@

\begin{marginfigure}[-8cm]
\includegraphics[width=1\textwidth]{PTS-autocorlin}
\caption{Autocorrelation: linear dependency}
\label{fig:autocorlin}
\setfloatalignment{b}
\end{marginfigure}

If however we construct a time series based on IID-prinicples (e.g. where $y$ is chosen at random from a standard normal distribution) we see no autocorrelation at all (Figure~\ref{fig:autocornorm}), except for k=0.

<<label=autocornorm, fig=TRUE, include=FALSE, echo=FALSE>>=
n <- 1000
autocornorm <- data.frame(t = seq(1,n), y = rnorm(n, 0, 1))
auto <- data.frame(lag = acf(autocornorm$y, lag.max=10, plot = FALSE)$lag, 
                   acf = acf(autocornorm$y, lag.max=10, plot = FALSE)$acf)
ci.high <- qnorm(0.975)/sqrt(n)
ci.low <- - ci.high
ggplot(data = auto, aes(x = lag, y = acf)) +
        geom_segment(aes(xend = lag, yend = 0), alpha = 0.4, size = 2) +
        geom_hline(yintercept = 0) +
        geom_hline(yintercept = ci.high, linetype = "dashed", col = "blue") +
        geom_hline(yintercept = ci.low, linetype = "dashed", col = "blue") +
        scale_x_continuous(limits=c(0, 10), breaks = seq(0,10,1)) +
        scale_y_continuous(limits=c(-1, 1), breaks = seq(-1,1,0.1)) +
        labs(title = "Autocorrelation (IID-constructed)", x = "lag", y = "result") +
        JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{PTS-autocornorm}
\caption{Autocorrelation: IID-constructed}
\label{fig:autocornorm}
\setfloatalignment{b}
\end{marginfigure}

\subsection{Partial autocorrelation in time series}
If 
\begin{equation}
	y_{t} = y_{t-1} + b_{1} \quad or \quad y_{t-1}=y_{t}-b_{1}
\end{equation}

then we can say that \emph{the effect of the autocorrelation at lag1}\index{partial autocorrelation}\index{autocorrelation!partial} is that it deducts $b_{1}$ from the time series at lag0. When we apply this effect on the time series at lag1 is that we get a new time series ($t=3 \ldots n$)
\begin{equation}
	z_{t-2} = y_{t-1} - b_{1} = y_{t-2}
\end{equation}

which is equal to the time series at lag2. If we \emph{remove} this effect from the time series at lag2 we get:
\begin{equation}
	y_{t-2} - z_{t-2} = 0
\end{equation}

We define the \emph{partial autocorrelation at lag2} as the correlation that results \emph{after removing the effect} of the correlation at lag1. In this case it would be the correlation between $y_{t}$ and $y_{t-2}-z_{t-2}=0$ (for $t=1 \ldots n$), which would be equal to $0$.

A more general definition of the \emph{partial autocorrelation at lagk} as the correlation that results after removing the effect of any correlations due to the terms at shorter lags will give partial correlation = $0$ for all lagk's (Figure~\ref{fig:pautocorlin}).

<<label=pautocorlin, fig=TRUE, include=FALSE, echo=FALSE>>=
part <- data.frame(lag = pacf(autocorlin$y, lag.max=10, plot = FALSE)$lag, 
                   acf = pacf(autocorlin$y, lag.max=10, plot = FALSE)$acf)
ci.high <- qnorm(0.975)/sqrt(n)
ci.low <- - ci.high
ggplot(data = part, aes(x = lag, y = acf)) +
        geom_segment(aes(xend = lag, yend = 0), alpha = 0.4, size = 2) +
        geom_hline(yintercept = 0) +
        geom_hline(yintercept = ci.high, linetype = "dashed", col = "blue") +
        geom_hline(yintercept = ci.low, linetype = "dashed", col = "blue") +
        scale_x_continuous(limits=c(0, 10), breaks = seq(0,10,1)) +
        scale_y_continuous(limits=c(-1, 1), breaks = seq(-1,1,0.1)) +
        labs(title = "Partial autocorrelation (linear dependency)", x = "lag", y = "result") +
        JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{PTS-pautocorlin}
\caption{Partial autocorrelation: linear dependency}
\label{fig:pautocorlin}
\setfloatalignment{b}
\end{marginfigure}

There are different tests to check if a time series is stationary: the \emph{Ljung-Box}-test, the \emph{Augmented Dickey–Fuller (ADF)}-test, the \emph{Kwiatkowski-Phillips-Schmidt-Shin (KPSS)}-test.

\newpage
\section{Dependency and Patterns}

\subsection{Trend}\index{trend}

In \ref{subsec:autocor} we showed that when we use a global linear function in $t$ ($y_{t}=b_{0}+b_{1}t$) (Figure~\ref{fig:lintrend}), we can also construct the same time series with a recursion formula:
\begin{equation}
	y_{t}=0.2 + y_{t-1} \quad with \quad y_{1}=5.2
\label{eq:linrecursion}
\end{equation}

This recursion formula \ref{eq:linrecursion} shows that the values of $y_{t}$ are \emph{dependent} of each other: there is autocorrelation and therefore this time series is \emph{not stationary}.

<<label=lintrend, fig=TRUE, include=FALSE, echo=FALSE>>=
n <- 20
lintrend <- data.frame(t = seq(1, n, 1), 
                       y = 0)
lintrend$y <- 5 + 0.2*lintrend$t
ggplot(data = lintrend, aes(x = t, y = y)) +
        geom_point(size = 2) +
        scale_x_continuous(limits=c(0, n), breaks = seq(0,n,1)) +
        scale_y_continuous(limits=c(0, 10), breaks = seq(0,10,1)) +
        labs(title = "Time series based on a linear function", x = "t", y = "result") +
        JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{PTS-lintrend}
\caption{Linear pattern}
\label{fig:lintrend}
\setfloatalignment{b}
\end{marginfigure}

With a small modification of the recursion formula (the co\"{e}ffici\"{e}nt of $y_{t-1}$) we can get other patterns: equation~\ref{eq:exp} gives us the same result as the non-linear function $y=-1 + 6.(1.2)^{t}$ ( Figure~\ref{fig:nonlintrend}-left). 
\begin{equation}
	y_{t} = 0.2 + 1.2y_{t-1}
\label{eq:exp}
\end{equation}

Recursive fuction~\ref{eq:asympt} gives the same result as the non-linear function $y=1 + 4.(0.8)^{t}$ (Figure~\ref{fig:nonlintrend}-right).
\begin{equation}
	y_{t} = 0.2 + 0.8y_{t-1}
\label{eq:asympt}
\end{equation}

<<label=nonlintrend, fig=TRUE, include=FALSE, echo=FALSE>>=
n <- 20
pattern <- data.frame(t = seq(1, n, 1),
                      y1 = 0,
                      y2 = 0)
pattern$y1 <- -1 + 6*(1.2^pattern$t)
pattern$y2 <- 1 + 4*(0.8^pattern$t)
p1 <- ggplot(data = pattern) +
        geom_point( aes(x = t, y = y1), size = 2, col = "red") +
        scale_x_continuous(limits=c(0, n), breaks = seq(0, n, 2)) +
        scale_y_continuous(limits=c(0, 150), breaks = seq(0, 150, 10)) +
        labs(title = "Non-linear pattern", x = "t", y = "result") +
        JT.theme
p2 <- ggplot(data = pattern) +
        geom_point( aes(x = t, y = y2), size = 2, col = "blue") +
        scale_x_continuous(limits=c(0, n), breaks = seq(0, n, 2)) +
        scale_y_continuous(limits=c(0, 5), breaks = seq(0, 5, 1)) +
        labs(title = "Non-linear pattern", x = "t", y = "result") +
        JT.theme
grid.arrange(p1, p2, nrow = 1)
@

\begin{figure}
\includegraphics[width=0.8\textwidth]{PTS-nonlintrend}
\caption{Non-linear patterns}
\label{fig:nonlintrend}
\setfloatalignment{b}
\end{figure}

It seems that if we see a general trend within a time series, it should be possible to find an appropriate function to describe this trend. For a real time series the measurements will probably not lie exactly on this trend function, but there are methods (e.g. \emph{ordinary least squares method}) that give us the functions parameters that minimize the difference between the actual measurement and the value that we calculate using the trend function. The most obvious method is \emph{linear regression}.

\subsection{Finding the trend with linear regression methods\index{linear regression}}

Figure~\ref{fig:polytrend} shows a time series and we could think that a quadratic function would be a good model for the trend. We can fit a quadratic curve through the datapoints in Figure~\ref{fig:polytrend}. 

<<label=polytrend, fig=TRUE, include=FALSE, echo=FALSE>>=
pattern$y1noise <- pattern$y1 + rnorm(n, 0, 3)
coef <- tidy(lm(y1noise ~ t + I(t^2), data = pattern))$estimate
pattern$fity <- coef[1] +
              coef[2]*pattern$t +
              coef[3]*(pattern$t)^2
ggplot(data = pattern) +
        geom_point(aes(x = t, y = y1noise), size = 2, col = "red") +
        geom_line(aes(x = t, y = fity)) +
        scale_x_continuous(limits=c(0, n), breaks = seq(0, n, 2)) +
        scale_y_continuous(limits=c(0, 150), breaks = seq(0, 150, 10)) +
        labs(title = "Non-linear pattern: fitted trend - quadratic", x = "t", y = "result") +
        JT.theme
@

\begin{marginfigure}[3cm]
\includegraphics[width=1\textwidth]{PTS-polytrend}
\caption{Quadratic trendline}
\label{fig:polytrend}
\setfloatalignment{b}
\end{marginfigure}

Our trend function in Figure~\ref{fig:polytrend} is good, but not great. Of course, other choices for the trend function are possible: we could try an exponential function etc. But in the end we could find a function that comes close to the measurements. It would be very tempting to use this function to make forecasts. However there are two serious objections to using this approach for forecasting:
\begin{itemize}
	\item Regression results are useful to make predictions \emph{within the range where the regression is made}. Forecasting is per definition about time points into the future, which means that we have to \emph{extrapolate}\index{extrapolation} outside the range. Trend functions obtained with linear regression will probably behave very well within the range, but can fail spectacularly outside the range.
	\item Linear regression methods are based on assumptions. One of them is that the \emph{residuals}, the difference between actual and calculated value, are \emph{independent}\index{independency!residuals} and that the variance of the residuals is constant. In other words: the time series of the residuals should be \emph{second order stationary}. This is usually not true, and the main cause of this is \emph{autocorrelation}.
\end{itemize}

\subsection{Autocorrelation of the residuals}

In Figure~\ref{fig:lintrend2} we have a time series with a perfect linear trend (red). This is very unrealistic because we know that every time series is only one of many possible representations (because $Y_{t}$ is a chance variable). So we should add some chance element into the definition of the time series. When we use linear regression we assume that the chance element for each measurement $y_{t}$ is independently taken from a (normal) distribution with mean $\mu=0$ and a constant variance $\sigma^{2}$:

\begin{equation}
	y = b_{0} + b_{1}t + \epsilon \quad with \quad \epsilon=N(0,\sigma)
\end{equation}

The first terms of time series that follows from this are:
\begin{equation}
	\begin{split}
	  y_{1} &= b_{0}+b_{1}+\epsilon_{1}(0,\sigma) \\
	  y_{2} &= b_{0}+2b_{1}+\epsilon_{2}(0,\sigma) \\
	  y_{3} &= b_{0}+3b_{1}+\epsilon_{3}(0,\sigma) \\
	  \ldots & \\
	  y_{t} &= b_{0}+tb_{1}+\epsilon_{t}(0,\sigma)
	\end{split}
\label{eq:cnsterr}
\end{equation}

which we see in Figure~\ref{fig:lintrend2} in blue.
<<label=lintrend2, fig=TRUE, include=FALSE, echo=FALSE>>=
set.seed(2017)
n <- 50
lintrend2 <- data.frame(t = seq(1, n, 1), 
                       y = 0,
                       ynoise = 0,
                       residnoise = 0,
                       ynoiserec = 0,
                       ynoiserec2 = 0,
                       ynoiserec3 = 0,
                       residnoiserec =0)
sigma <- 1
eps <- rnorm(n, 0, sigma)
lintrend2$y[1] <- 5 
for (i in seq(2, n, 1)) {lintrend2$y[i] = lintrend2$y[i-1] + 0.2}
lintrend2$ynoise <- lintrend2$y + eps
lintrend2$residnoise <- lintrend2$ynoise - lintrend2$y
lintrend2$ynoiserec[1] <- 5 + eps[1]
for (i in seq(2, n, 1)) {lintrend2$ynoiserec[i] = lintrend2$ynoiserec[i-1] + 0.2 + eps[i]}
eps2 <- rnorm(n, 0, sigma)
lintrend2$ynoiserec2[1] <- 5 + eps2[1]
for (i in seq(2, n, 1)) {lintrend2$ynoiserec2[i] = lintrend2$ynoiserec2[i-1] + 0.2 + eps2[i]}
eps3 <- rnorm(n, 0, sigma)
lintrend2$ynoiserec3[1] <- 5 + eps3[1]
for (i in seq(2, n, 1)) {lintrend2$ynoiserec3[i] = lintrend2$ynoiserec3[i-1] + 0.2 + eps3[i]}
lintrend2$residnoiserec <- lintrend2$ynoiserec - lintrend2$y
ggplot(data = lintrend2) +
        geom_line(aes(x = t, y = y), size = 1, col = "red") +
        geom_point(aes(x = t, y = ynoise), size = 2, col = "blue") +
        geom_point(aes(x = t, y = ynoiserec), size = 6, col = "black", shape = "+") +
        scale_x_continuous(limits=c(0, n), breaks = seq(0,n,5)) +
        scale_y_continuous(limits=c(0, 20), breaks = seq(0,20,1)) +
        labs(title = "Time series with error term", x = "t", y = "result") +
        JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{PTS-lintrend2}
\caption{Time series with error term}
\label{fig:lintrend2}
\setfloatalignment{b}
\end{marginfigure}

However, when we construct the time series with the recursion formula \emph{using the same values for $\epsilon$ for each value of $t$} we get
\begin{equation}
  \begin{split}
	  y_{1} &= b_{0} + b_{1} + \epsilon_{1}(0,\sigma) \\
	  y_{2} &= y_{1} + b_{1} + \epsilon_{2}(0,\sigma) = b_{0} + 2b_{1} + \epsilon_{1}(0,\sigma) + \epsilon_{2}(0,\sigma) \\
	  y_{3} &= y_{2} + b_{1} + \epsilon_{3}(0,\sigma) = b_{0} + 3b_{1} + \epsilon_{1}(0,\sigma) + \epsilon_{2}(0,\sigma) + \epsilon_{3}(0,\sigma)\\
	  \ldots & \\
	  y_{t} &= y_{t-1} + b_{1} + \epsilon_{t}(0,\sigma) = b_{0} + tb_{1} + \epsilon_{1}(0,\sigma) + \epsilon_{2}(0,\sigma) + \ldots + \epsilon_{t}(0,\sigma) 
	\end{split}
\label{eq:recurerror}
\end{equation}

which is quite a different time series (black). We can see in (\ref{eq:recurerror}) that each value $y_{t}$ has its own error $\epsilon_{t}$ but it also carries with it the error of $y_{t-1}$, and the errors in all previous values. This means that the error at time point $t$ will depend on previous errors. In other words: there is \emph{autocorrelation} within the time series of the errors. The variance will steadily increase and we can write:
\begin{equation}
	y_{t} = b_{0} + tb_{1} + \epsilon_{t}(0,\sqrt{t}\sigma)
\end{equation}

In Figure~\ref{fig:lintrend3} we have two other time series constructed with (\ref{eq:recurerror}) and we can see that the variance is larger that for the ''blue" time series.

<<label=lintrend3, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = lintrend2) +
        geom_line(aes(x = t, y = y), size = 1, col = "red") +
        geom_point(aes(x = t, y = ynoise), size = 1.5, col = "blue") +
        geom_point(aes(x = t, y = ynoiserec), size = 3, col = "black", shape = 3) +
        geom_point(aes(x = t, y = ynoiserec2), size = 3, col = color[2], shape = 4) +
        geom_point(aes(x = t, y = ynoiserec3), size = 3, col = color[3], shape = 5) +
        scale_x_continuous(limits=c(0, n), breaks = seq(0,n,5)) +
        scale_y_continuous(limits=c(0, 20), breaks = seq(0,20,1)) +
        labs(title = "Time series with error term", x = "t", y = "result") +
        JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{PTS-lintrend3}
\caption{Time series with error term}
\label{fig:lintrend3}
\setfloatalignment{b}
\end{marginfigure}

We can calculate the \emph{residuals}, the difference between the actual value and the calculated values from the linear regression. For the time series constructed with equation (\ref{eq:cnsterr}) we get Figure~\ref{fig:cnsterr}: the residuals on the left, the autocorrelation for different lags on the right. There is a hint of autocorrelation at lag1 but it just scrapes the lower confidence interval.

<<label=cnsterr, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- ggplot(data = lintrend2) +
        geom_point(aes(x = t, y = residnoise), size = 1.5, col = "blue") +
        scale_x_continuous(limits=c(0, n), breaks = seq(0,n,5)) +
        scale_y_continuous(limits=c(-3, 3), breaks = seq(-2, 2, 1)) +
        labs(title = "Residuals of time series\n(based on linear regression)", x = "t", y = "result") +
        JT.theme
autocor <- data.frame(lag = acf(lintrend2$residnoise, lag.max=10, plot = FALSE)$lag, 
                   acf = acf(lintrend2$residnoise, lag.max=10, plot = FALSE)$acf)
ci.high <- qnorm(0.975)/sqrt(n)
ci.low <- - ci.high
p2 <- ggplot(data = autocor, aes(x = lag, y = acf)) +
        geom_segment(aes(xend = lag, yend = 0), alpha = 0.4, size = 2) +
        geom_hline(yintercept = 0) +
        geom_hline(yintercept = ci.high, linetype = "dashed", col = "blue") +
        geom_hline(yintercept = ci.low, linetype = "dashed", col = "blue") +
        scale_x_continuous(limits=c(0, 10), breaks = seq(0,10,1)) +
        scale_y_continuous(limits=c(-1, 1), breaks = seq(-1,1,0.1)) +
        labs(title = "Autocorrelation residuals", x = "lag", y = "acf") +
        JT.theme
grid.arrange(p1, p2, nrow=1)
@

\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{PTS-cnsterr}
\caption{Residuals }
\label{fig:cnsterr}
\setfloatalignment{b}
\end{center}
\end{figure}

For the time series constructed with equation~\ref{eq:recurerror} we get Figure~\ref{fig:cumerr}. Now there is clear autocorrelation within the residuals at lag1 and lag2.

<<label=cumerr, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- ggplot(data = lintrend2) +
        geom_point(aes(x = t, y = residnoiserec), size = 1.5, col = "blue") +
        scale_x_continuous(limits=c(0, n), breaks = seq(0,n,5)) +
        scale_y_continuous(limits=c(-3, 3), breaks = seq(-2, 2, 1)) +
        labs(title = "Residuals of time series\n(based on autocorrelation)", x = "t", y = "result") +
        JT.theme
autocor <- data.frame(lag = acf(lintrend2$residnoiserec, lag.max=10, plot = FALSE)$lag, 
                   acf = acf(lintrend2$residnoiserec, lag.max=10, plot = FALSE)$acf)
ci.high <- qnorm(0.975)/sqrt(n)
ci.low <- - ci.high
p2 <- ggplot(data = autocor, aes(x = lag, y = acf)) +
        geom_segment(aes(xend = lag, yend = 0), alpha = 0.4, size = 2) +
        geom_hline(yintercept = 0) +
        geom_hline(yintercept = ci.high, linetype = "dashed", col = "blue") +
        geom_hline(yintercept = ci.low, linetype = "dashed", col = "blue") +
        scale_x_continuous(limits=c(0, 10), breaks = seq(0,10,1)) +
        scale_y_continuous(limits=c(-1, 1), breaks = seq(-1,1,0.1)) +
        labs(title = "Autocorrelation residuals", x = "lag", y = "acf") +
        JT.theme
grid.arrange(p1, p2, nrow=1)
@

\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{PTS-cumerr}
\caption{Residuals: autocorrelated time series}
\label{fig:cumerr}
\setfloatalignment{b}
\end{center}
\end{figure}


The presence of autocorrelation within a time series seems to be a great nuisance. However, we can make good use of it when our time series has the underlying (linear) structure:
\begin{equation}
	y_{t}=y_{t-1} + b_{1} + \epsilon_{t}(0,\sigma)
\label{eq:linstruc}
\end{equation}

We can create a new time series $z_{t}$ by making the difference between $y_{t}$ and $y_{t-1}$, called the \emph{once-differenced} time series\index{differenced time series}\index{time series!differenced}:
\begin{equation}
	z_{t}=y_{t} - y_{t-1} = b_{1} + \epsilon_{t}(0,\sigma) = \epsilon_{t}(b_{1},\sigma)
\label{eq:oncediff}
\end{equation}

We can see from equation (\ref{eq:oncediff}) that the \emph{once-differenced} time series $z_{t}$ is a stationary time series with constant mean $\mu=b_{1}$ and constant standard deviation $\sigma=\sigma$. And this information can be used to make our first forecast.

\newpage
\subsection{A first try at forecasting}
\label{subsec:firstforecast}

We start with the time series in Figure~\ref{fig:lintrend4}\sidenote{the black time series from Figure~\ref{fig:lintrend2}}.

<<label=lintrend4, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = lintrend2) +
        geom_point(aes(x = t, y = ynoiserec), size = 3, col = "black", shape = 3) +
        scale_x_continuous(limits=c(0, n), breaks = seq(0,n,5)) +
        scale_y_continuous(limits=c(0, 20), breaks = seq(0,20,1)) +
        labs(title = "Given time series", x = "t", y = "result") +
        JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{PTS-lintrend4}
\caption{Given time series}
\label{fig:lintrend4}
\setfloatalignment{b}
\end{marginfigure}

With some goodwill we can detect a linear trend, which means that the underlying recursive formula associated with this time series could be of the form
\begin{equation}
	y_{t} = y_{t-1} + b_{1} \quad t=1 \ldots n
\end{equation}

We calculate the once-differenced time series $z_{t}$:
\begin{equation}
	z_{t} = y_{t} - y_{t-1} \quad t=2 \ldots n
\end{equation}

The values of $z_{t}$ are:
\begin{equation}
	\begin{split}
	  z_{1} &= NA  \\
	  z_{2} &= y_{2} - y_{1} = b_{1}+\epsilon_{2}(0,\sigma) = \epsilon_{2}(b_{1},\sigma) \\
	  z_{3} &= y_{3} - y_{2} = b_{1}+\epsilon_{3}(0,\sigma) = \epsilon_{3}(b_{1},\sigma) \\
	  \ldots & \\
	  z_{t} &= y_{t} - y_{t-1} = b_{1}+\epsilon_{t}(0,\sigma) = \epsilon_{t}(b_{1},\sigma) \\
	\end{split}
\end{equation}


<<label=lintrend5, fig=TRUE, include=FALSE, echo=FALSE>>=
lintrend$div[1] <- NA
lintrend2$div[2:n] <- diff(lintrend2$ynoiserec)
mudiv <- mean(lintrend2$div[2:n])
sddiv <- sd(lintrend2$div[2:n])
p1 <- ggplot(data = lintrend2) +
        geom_point(aes(x = t, y = div), size = 3, col = "black", shape = 3) +
        scale_x_continuous(limits=c(0, n), breaks = seq(0,n,5)) +
        scale_y_continuous(limits=c(-3, 3), breaks = seq(-3,3,1)) +
        labs(title = "Time series z(t)=y(t)-y(t-1)", x = "t", y = "result") +
        JT.theme
autocor <- data.frame(lag = acf(lintrend2$div[2:n], lag.max=10, plot = FALSE)$lag, 
                   acf = acf(lintrend2$div[2:n], lag.max=10, plot = FALSE)$acf)
ci.high <- qnorm(0.975)/sqrt(n)
ci.low <- - ci.high
p2 <- ggplot(data = autocor, aes(x = lag, y = acf)) +
        geom_segment(aes(xend = lag, yend = 0), alpha = 0.4, size = 2) +
        geom_hline(yintercept = 0) +
        geom_hline(yintercept = ci.high, linetype = "dashed", col = "blue") +
        geom_hline(yintercept = ci.low, linetype = "dashed", col = "blue") +
        scale_x_continuous(limits=c(0, 10), breaks = seq(0,10,1)) +
        scale_y_continuous(limits=c(-1, 1), breaks = seq(-1,1,0.1)) +
        labs(title = "Autocorrelation of z(t)", x = "lag", y = "acf") +
        JT.theme
grid.arrange(p1, p2, nrow=1)
@

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{PTS-lintrend5}
\caption{Differenced time series}
\label{fig:lintrend5}
\setfloatalignment{b}
\end{center}
\end{figure}

If $z_{t}$ is a stationary time series,we expect its values to be randomly distributed around a constant level $b_{1}$, and Figure\ref{fig:lintrend5}-left confirms this. We can also check the autocorrelation graph (Figure\ref{fig:lintrend5}-right) and we see that there is probably no autocorrelation. From the once-differenced time series $z_{t}$ we can calculate the mean ($\mu=b_{1}=$\Sexpr{round(mudiv,3)}) and the standard deviation ($\sigma=$\Sexpr{round(sddiv,3)}). 

Our first tentative forecast (Figure~\ref{fig:lintrend6}) for the next k\sidenote{here k=5} time points goes as follows:
\begin{itemize}
  \item the trendline (red and dashed) is given by $y_{i} = y_{i-1} + b_{1} = y_{i-1}$ + \Sexpr{round(mudiv,3)}  for i=1, $\ldots$ , n
  \item the starting point of the trendline is the ''intercept" of the linear regression through the time series data
	\item the forecasted values are given by $y_{i} = y_{i-1} + b_{1} = y_{i-1}$ + \Sexpr{round(mudiv,3)}  for i=(n+1), $\ldots$ , (n+k)
	\item the starting point of the forecast $y_{n+1}$ is equal to the last point of the trendline + \Sexpr{round(mudiv,3)}
	\item the 95\% upper confidence level is given by $y_{i}$ + 1.96$\sqrt{(i-n)}\sigma$ = $y_{i}$ + 1.96$\sqrt{i-n}$\Sexpr{round(sddiv,3)}  for i=(n+1), $\ldots$ , (n+k)
	\item the 95\% lower confidence level is given by $y_{i}$ - 1.96$\sqrt{(i-n)}\sigma$ =$y_{i}$ - 1.96$\sqrt{i-n}$\Sexpr{round(sddiv,3)}  for i=(n+1), $\ldots$ , (n+k)
\end{itemize}

<<label=lintrend6, fig=TRUE, include=FALSE, echo=FALSE>>=
lintrend2$ytrend[1] <- lm(ynoiserec ~t, data=lintrend2)$coefficients[1]
for (i in (seq(2, n, 1))) {lintrend2$ytrend[i] = lintrend2$ytrend[i-1] + mudiv}
k <- 5
lintrend2.forecast <- data.frame(i = seq(1, k, 1), t = 0, y = 0, ylow = 0, yhigh = 0)
lintrend2.forecast$t <- lintrend2.forecast$i + n
lintrend2.forecast$y[1] <- lintrend2$ytrend[n] + mudiv
lintrend2.forecast$ylow[1] <- lintrend2.forecast$y[1] - 1.96*sddiv
lintrend2.forecast$yhigh[1] <- lintrend2.forecast$y[1] + 1.96*sddiv
for (i in seq(2, k, 1)) {
  lintrend2.forecast$y[i] <- lintrend2.forecast$y[i-1] + mudiv
  lintrend2.forecast$ylow[i] <- lintrend2.forecast$y[i] - 1.96*sqrt(i)*sddiv
  lintrend2.forecast$yhigh[i] <- lintrend2.forecast$y[i] + 1.96*sqrt(i)*sddiv
}
ggplot() +
        geom_point(data = lintrend2, aes(x = t, y = ynoiserec), size = 3, col = "black", shape = 3) +
        geom_line(data = lintrend2, aes(x = t, y = ytrend), size = 0.5, col = "red", linetype = "dashed") +
        geom_line(data = lintrend2.forecast, aes(x = t, y = y), size = 1, col = "blue") +
        geom_ribbon(data = lintrend2.forecast, aes(x = t, ymin = ylow, ymax = yhigh), fill = "red", alpha = 0.3) +
        scale_x_continuous(limits=c(0, (n+k)), breaks = seq(0,(n+k),5)) +
        scale_y_continuous(limits=c(0, 25), breaks = seq(0,25,1)) +
        labs(title = "Forecast of time series", x = "t", y = "result") +
        JT.theme
@

\begin{figure}
\includegraphics[width=1\textwidth]{PTS-lintrend6}
\caption{Forecast for given time series}
\label{fig:lintrend6}
\setfloatalignment{b}
\end{figure}
\medskip
It is clear that this was an easy example: we started from a self-constructed time series based on a linear trend, so it should not come as a great surprise that our assumption of a linear trend works well! When we work with a given time series we do not know anything about the trend.  Visualising the time series will give us some idea but still we have to make assumptions about the trend:
\begin{itemize}
	\item is it linear?
	\item a polynomial? of what degree?
	\item an exponential function?
	\item a logarithmic function?
	\item other?
\end{itemize}
\medskip
So we could ask: is there a better way to find the trend?

In our example we also decided that there was no autocorrelation remaining in the residuals, while there was some sign of autocorrelation at lag1 and lag2. A second question is: can we build a trend model that further reduces serial correlation within the residuals?

\subsection{Finding the trend from the data}

The basis of our forecast in \ref{subsec:firstforecast} was that we started from the assumption that our time series had a linear trend, given by:
\begin{equation}
	y_{t} = y_{t-1} + b_{1} + \epsilon_{t}(0,\sigma)
\label{eq:lin1}
\end{equation}

From that assumption it followed that the once-differenced time series $z_{t}$ was stationary, and that the mean of $z_{t}$ was equal to $b_{1}$ and the standard deviation of $z_{t}$ was equal to $\sigma$. This gave us all the information we needed to construct the trend line ($y_{t}=y_{t-1} + b_{1}$), the forecasted values, and the confidence interval for these forecasts.

We can rewrite recursion formula (\ref{eq:lin1}) as
\begin{equation}
	y_{t-1} = y_{t-2} + b_{1} + \epsilon_{t-1}(0,\sigma)
\end{equation}

From this we find $b_{1}$ as
\begin{equation}
	b_{1} = y_{t-1} - y_{t-2} - \epsilon_{t-1}(0,\sigma)
\end{equation}

and thus
\begin{equation}
\begin{split}
	y_{t} &= 2y_{t-1} - y_{t-2} = 2y_{t-1} - y_{t-2} - \epsilon_{t-1}(0,\sigma) + \epsilon_{t}(0,\sigma) + \epsilon_{t}(0,\sigma) \\
	&= 2y_{t-1} - y_{t-2} + \epsilon_{t}(0,\sqrt{2}\sigma)
	\end{split}
\end{equation}

Instead of imposing this linear relation upon our data, we could start from the general expression
\begin{equation}
	y_{t} = \alpha_{1}y_{t-1} + \alpha_{2}y_{t-2} + \epsilon_{t}(0,\sigma)
\label{eq:autoregression2}
\end{equation}

and determine the best values for $\alpha_{1}$ and $\alpha_{2}$ from our data. This is called an \emph{autoregressive model of order 2} (\emph{AR2-model})\index{autoregressive model}\index{autoregression!autoregressive model}\index{AR-model}\index{autoregression!AR-model}. 
\newpage
We can find the coefficients $\alpha_{i}$ of AR2-model (\ref{eq:autoregression2}) by linear regression of $y_{t}$ with $y_{t-1}$ and $y_{t-2}$\sidenote{detail: put a ''0" as first term of the linear regression to make sure that the regression equation does not produce a constant term}

<<>>=
mod.ar2 <- lm(lintrend2$ynoiserec[3:n] ~ 
                0 + 
                lintrend2$ynoiserec[2:(n-1)] + 
                lintrend2$ynoiserec[1:(n-2)])
@
\bigskip
<<>>=
tidy(mod.ar2)
@
\bigskip
<<>>=
glance(mod.ar2)
@
\bigskip

<<label=modar2, fig=TRUE, include=FALSE, echo=FALSE>>=
# calculate the trendline
a1 <- tidy(mod.ar2)$estimate[1]
a2 <- tidy(mod.ar2)$estimate[2]
lintrend2$ytrendar2[1:2] <- NA
lintrend2$ytrendar2[3:n] <- mod.ar2$fitted.values
# calculate residuals
lintrend2$residar2 <- lintrend2$ynoiserec - lintrend2$ytrendar2
# find the autocorrelation within the residuals
autoar2 <- data.frame(lag = acf(lintrend2$residar2[3:n], lag.max=10, plot = FALSE)$lag, 
                   acf = acf(lintrend2$residar2[3:n], lag.max=10, plot = FALSE)$acf)
ci.high <- qnorm(0.975)/sqrt(n)
ci.low <- - ci.high
# calculate the standard deviation of the residuals
sdres <- sd(lintrend2$residar[3:n])
# calculate the forecasted values (k=5)
k <- 5
lintrend2.forecast.ar2 <- data.frame(i = seq(1, k, 1), t = 0, y = 0, ylow = 0, yhigh = 0)
lintrend2.forecast.ar2$t <- lintrend2.forecast.ar2$i + n
lintrend2.forecast.ar2$y[1] <- a1*lintrend2$ynoiserec[n] + a2*lintrend2$ynoiserec[n-1]
lintrend2.forecast.ar2$y[2] <- a1*lintrend2.forecast.ar2$y[1] + a2*lintrend2$ynoiserec[n]
for (i in seq(3, k, 1)) {
  lintrend2.forecast.ar2$y[i] <- a1*lintrend2.forecast.ar2$y[i-1] + 
    a2*lintrend2.forecast.ar2$y[i-2]
}
for (i in seq(1, k, 1)) {
  lintrend2.forecast.ar2$ylow[i] <- lintrend2.forecast.ar2$y[i] - 1.96*sqrt(i)*sdres
  lintrend2.forecast.ar2$yhigh[i] <- lintrend2.forecast.ar2$y[i] + 1.96*sqrt(i)*sdres
}
p1 <- ggplot() +
        geom_point(data = lintrend2, aes(x = t, y = ynoiserec), 
                   size = 3, col = "black", shape = 3) +
        geom_line(data = lintrend2, aes(x = t, y = ytrendar2), 
                  size = 0.5, col = "red", linetype = "dashed") +
        geom_line(data = lintrend2.forecast.ar2, aes(x = t, y = y), size = 1, col = "blue") +
        geom_ribbon(data = lintrend2.forecast.ar2, aes(x = t, ymin = ylow, ymax = yhigh), fill = "red", alpha = 0.3) +
        scale_x_continuous(limits=c(0, (n+k)), breaks = seq(0,(n+k),5)) +
        scale_y_continuous(limits=c(0, 27), breaks = seq(0,27,1)) +
        labs(title = "AR2-model of time series", x = "t", y = "result") +
        JT.theme
p2 <- ggplot(data = autoar2, aes(x = lag, y = acf)) +
        geom_segment(aes(xend = lag, yend = 0), alpha = 0.4, size = 2) +
        geom_hline(yintercept = 0) +
        geom_hline(yintercept = ci.high, linetype = "dashed", col = "blue") +
        geom_hline(yintercept = ci.low, linetype = "dashed", col = "blue") +
        scale_x_continuous(limits=c(0, 10), breaks = seq(0,10,1)) +
        scale_y_continuous(limits=c(-1, 1), breaks = seq(-1,1,0.1)) +
        labs(title = "Autocorrelation\nresiduals AR2-model", x = "lag", y = "result") +
        JT.theme
grid.arrange(p1, p2, nrow = 1)
@

\begin{figure}
\includegraphics[width=1\textwidth]{PTS-modar2}
\caption{AR2-model of time series}
\label{fig:modar2}
\setfloatalignment{b}
\end{figure}
\medskip

We can see that there is autocorrelation of $y_{t}$ with $y_{t-1}$ and $y_{t-2}$ because all p-values are well below the 5\% treshold. We can now construct the trend line based on this \emph{AR2-model}, we can forecast into the future and determine a 95\%-confidence interval for these forecasts (Figure~\ref{fig:modar2}-left). 

Autoregressive models usually do not produce a stationary time series. Howeve,: that is not important because we use these models to simulate real time series, and real time series are mostly non-stationary. What we are aiming for is that the residuals (difference between the actual times series and the model) form a stationary time series and this we have achieved with the AR-2 model (Figure~\ref{fig:modar2}-right). Only the autocorrelation for lag0 remains; all others are now well within the confidence interval and play no role.

\subsection{Higher order autoregression models}

We can go further and generalise this to an \emph{autoregression model of order p}:
\begin{equation}
	y_{t} = \alpha_{1}y_{t-1} + \alpha_{2}y_{t-2} + \ldots + \alpha_{p}y_{t-p} + \epsilon_{t}(0,\sigma)
\label{eq:autoregressionp}
\end{equation}

The following tables give the results of the autoregression models of order 3 (AR3) and 4 (AR4):
<<echo=FALSE>>=
mod.ar3 <- lm(lintrend2$ynoiserec[4:n] ~ 
                0 + 
                lintrend2$ynoiserec[3 : (n-1)] + 
                lintrend2$ynoiserec[2 : (n-2)] +
                lintrend2$ynoiserec[1 : (n-3)])
mod.ar4 <- lm(lintrend2$ynoiserec[5:n] ~ 
                0 + 
                lintrend2$ynoiserec[4 : (n-1)] + 
                lintrend2$ynoiserec[3 : (n-2)] +
                lintrend2$ynoiserec[2 : (n-3)] +
                lintrend2$ynoiserec[1 : (n-4)])
@
\bigskip
<<>>=
tidy(mod.ar3)
@
\bigskip
<<>>=
tidy(mod.ar4)
@
\bigskip

Increasing the order of the autocorrelation model does not give better results: we can see that in the AR-3 model, the p-value for the third component is much above the treshold (5\%) for the p-value. The same is true in the AR-4 model for the third and fourth component. It seems that the AR-2 model is the right one.

\subsection{Finding the optimal model: training and validation}

As with every statistical model, we should not overdo the number of terms. Increasing the order of the AR-model makes it more and more vulnerable to \emph{overfitting}\index{overfitting}. To judge which value of $p$ gives us the optimal model, we should use a criterion that judges how well our model fits the data. To compare different models (e.g. AR1, AR2, AR3 ...) we split our time series into two parts: a \emph{training set}\index{training set} and a \emph{validation set}\index{validation set}. Using the \emph{training set} we create the different models. We use these models to make forecasts for the \emph{validation set}. We compare the two and we can calculate different measures of accuracy: 
\begin{itemize}
    \item \emph{Mean Error}\index{mean error}\index{error!mean}\index{ME!mean error}: idem but without the absolute values. Gives an idea of under- or overprediction
     \begin{equation}
	    ME = \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} e_{i}
    \end{equation}
	\item \emph{Mean Absolute Error}\index{mean absolute error}\index{error!mean absolute error}\index{MAE!mean absolute error} (or Deviation) (MAE of MAD)
	  \begin{equation}
	    MAE = \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} |e_{i}|
    \end{equation}
  \item \emph{Mean Absolute Percentage Error}\index{mean absolute percentage error}\index{error!mean absolute percentage error}\index{MAPE!mean absolute percentage error}: relative absolute error. Used when comparing performance across time-series with different scales
     \begin{equation}
	    MAPE = \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}}| \frac{e_{i}}{y_{i}}*100 | 
    \end{equation}
  \item \emph{Root Mean Square Error}\index{root mean square error}\index{error!root mean square error}\index{RMSE!root mean square error}
    \begin{equation}
	    RMSE = \sqrt{ \frac{1}{n_{Valid}}\sum_{1}^{n_{Valid}} e_{i}^{2} }
    \end{equation}
\end{itemize}

The \textbf{\textsf{R}} programming language has the function \textit{accuracy} that calculates these metrics so that we can compare the performance of the different models.

An alternative is the \textbf{\textsf{R}}-function \textit{ar} that gives the order and the $\alpha$-coefficients of the optimal autoregression model for a given time series. You have to make a judgment about the optimisation criterion that the function should use. Different choices will give different results, and the complexities of chosing the right one is outside the scope of this article. 
\newpage
As an example we use the \url{csiro_alt_gmsl_mo_2015} data set.\sidenote[][0cm]{Global Average Absolute Sea Level Change, 1880-2014 from the US Environmental Protection Agency using data from CSIRO, 2015; NOAA, 2015. This time series contains “cumulative changes in sea level for the world’s oceans from Januari 1993 up to Februari 2015, based on a combination of long-term tide gauge measurements and recent satellite measurements. It shows average absolute sea level change, which refers to the height of the ocean surface, regardless of whether nearby land is rising or falling. Satellite data are based solely on measured sea level, while the long-term tide gauge data include a small correction factor because the size and shape of the oceans are changing slowly over time. (On average, the ocean floor has been gradually sinking since the last Ice Age peak, 20,000 years ago.) Name: CSIRO (Commonwealth Scientific and Industrial Research Organization)}. Let us take the data for the years 2003 to 2007 (both included) (Figure~\ref{fig:sea1}): this is our \emph{training set}. We will use the resulting AR-model and to make a forecast for the year 2008. This will be our \emph{validation set}

<<label=sea1, fig=TRUE, include=FALSE, echo=FALSE>>=
sea <- read_csv("Data/csiro_alt_gmsl_mo_2015_csv.csv")
sea %>% dplyr::filter(year(Time) > 2002 & year(Time) < 2008) -> sea_train
ntrain <- nrow(sea_train)
sea %>% dplyr::filter(year(Time) == 2008) -> sea_valid
nvalid <- nrow(sea_valid)
sea_train$t <- seq(1, ntrain, 1)
sea_valid$t <- seq((ntrain +1), (ntrain + nvalid), 1)
ggplot() +
  geom_point(data = sea_train, aes(x = t, y = GMSL), size = 3, col = "black", shape = 3) +
  scale_x_continuous(limits=c(0, (ntrain + nvalid)), breaks = seq(0,(ntrain + nvalid),5)) +
  scale_y_continuous(limits=c(30, 60), breaks = seq(30, 60, 2)) +
  labs(title = "Sea level time series\n(2003-2007)", x = "t", y = "GMSL") +
  JT.theme
@

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{PTS-sea1}
\caption{Sea level time series (2003-2007)}
\label{fig:sea1}
\setfloatalignment{b}
\end{marginfigure}
\medskip

We construct a number of autoregression models of different order p, and we see that from the AR-3 model there is no significant regression on $y_{t-3}$:
<<>>=
sea.ar1 <- lm(GMSL[2:ntrain] ~  0 + GMSL[1:(ntrain - 1)],
              data = sea_train)
tidy(sea.ar1)
@
\bigskip
<<>>=
sea.ar2 <- lm(GMSL[3:ntrain] ~  0 + GMSL[2:(ntrain - 1)] +
              GMSL[1:(ntrain - 2)],
              data = sea_train)
tidy(sea.ar2)
@
\bigskip
<<>>=
sea.ar3 <- lm(GMSL[4:ntrain] ~  0 + GMSL[3:(ntrain - 1)] +
              GMSL[2:(ntrain - 2)] +
              GMSL[1:(ntrain - 3)],
              data = sea_train)
tidy(sea.ar3)
@
\bigskip

So we choose the AR-2 model to construct the trendline, calculate the residuals and check the autocorrelations within the residuals (Figure~\ref{fig:seaar2})

<<label=seaar2, fig=TRUE, include=FALSE, echo=FALSE>>=
# calculate the trendline
sea_train$trend <- rep(0, ntrain)
a1 <- tidy(sea.ar2)$estimate[1]
a2 <- tidy(sea.ar2)$estimate[2]
sea_train$trend[1:2] <- NA
sea_train$trend[3:ntrain] <- sea.ar2$fitted.values
# calculate residuals
sea_train$resid <- rep(0, ntrain)
sea_train$resid <- sea_train$GMSL - sea_train$trend
# find the autocorrelation within the residuals
autosea <- data.frame(lag = acf(sea_train$resid[3:ntrain], lag.max=10, plot = FALSE)$lag, 
                   acf = acf(sea_train$resid[3:n], lag.max=10, plot = FALSE)$acf)
ci.high <- qnorm(0.975)/sqrt(ntrain)
ci.low <- - ci.high
# calculate the standard deviation of the residuals
sdres <- sd(sea_train$resid[3:n])
# calculate the forecasted values (k=5)
sea_valid$forecast <- rep(0, nvalid)
sea_valid$low <- rep(0, nvalid)
sea_valid$high <- rep(0, nvalid)
sea_valid$forecast[1] <- a1*sea_train$GMSL[ntrain] + a2*sea_train$GMSL[ntrain-1]
sea_valid$forecast[2] <- a1*sea_valid$forecast[1] + a2*sea_train$GMSL[ntrain]
for (i in seq(3, nvalid, 1)) {
  sea_valid$forecast[i] <- a1*sea_valid$forecast[i-1] + a2*sea_valid$forecast[i-1]
}
for (i in seq(1, nvalid, 1)) {
  sea_valid$low[i] <- sea_valid$forecast[i] - 1.96*sqrt(i)*sdres
  sea_valid$high[i] <- sea_valid$forecast[i] + 1.96*sqrt(i)*sdres
}
p1 <- ggplot() +
        geom_point(data = sea_train, aes(x = t, y = GMSL), 
                   size = 3, col = "black", shape = 3) +
        geom_point(data = sea_valid, aes(x = t, y = GMSL), 
                   size = 2, col = "red", shape = 16) +
        geom_line(data = sea_train, aes(x = t, y = trend), 
                  size = 0.5, col = "red", linetype = "dashed") +
        geom_line(data = sea_valid, aes(x = t, y = forecast), size = 1, col = "red") +
        geom_ribbon(data = sea_valid, aes(x = t, ymin = low, ymax = high), fill = "blue", alpha = 0.3) +
        scale_x_continuous(limits=c(0, (ntrain + nvalid)), breaks = seq(0,(ntrain + nvalid),5)) +
        scale_y_continuous(limits=c(25, 65), breaks = seq(25,65,2)) +
        labs(title = "Sea time series AR2-model\ntrend, forecast", x = "t", y = "GMSL") +
        JT.theme
p2 <- ggplot(data = autosea, aes(x = lag, y = acf)) +
        geom_segment(aes(xend = lag, yend = 0), alpha = 0.4, size = 2) +
        geom_hline(yintercept = 0) +
        geom_hline(yintercept = ci.high, linetype = "dashed", col = "blue") +
        geom_hline(yintercept = ci.low, linetype = "dashed", col = "blue") +
        scale_x_continuous(limits=c(0, 10), breaks = seq(0,10,1)) +
        scale_y_continuous(limits=c(-1, 1), breaks = seq(-1,1,0.1)) +
        labs(title = "Sea time series AR2-model\nautocorrelation of residuals", x = "lag", y = "result") +
        JT.theme
grid.arrange(p1, p2, nrow = 1)
@

\begin{figure}
\includegraphics[width=1\textwidth]{PTS-seaar2}
\caption{AR2-model of Sea time series}
\label{fig:seaar2}
\setfloatalignment{b}
\end{figure}
\medskip

We can see that (almost) all of the actual values in the validation period lie within the 95\%-confidence level around the forecasted values. The accuracy metrics of this model are:
<<>>=
accuracy(sea_valid$forecast, sea_valid$GMSL)
@

\newpage
Now we can use the \textit{ar}-function to find the \emph{optimal} model according to \textbf{\textsf{R}}\sidenote{we use the method of ''maximum likelihood estimation" (MLE)}:
<<>>=
sea.opt <- ar(sea_train$GMSL, method='mle')
sea.opt$order
sea.opt$ar
sea_valid$forecast.opt <- predict(sea.opt, n.ahead = nvalid)$pred
@

The order remains 2, but the coefficients are a little bit different. We can calculate the forecasted values for 2008 with this model, and the accuracy of this model compared with the actual values:

<<>>=
accuracy(sea_valid$forecast.opt, sea_valid$GMSL)
@

We're not doing bad: our accuracy is better than the optimal solution for all metrics, and our forecasts for 2008 are better (Figure~\ref{fig:seaaropt})!

<<label=seaaropt, fig=TRUE, include=FALSE, echo=FALSE>>=
# calculate the trendline
sea_train$trend.opt<- rep(0, ntrain)
sea_train$trend.opt[1:2] <- NA
sea_train$trend.opt[3:ntrain] <- fitted(sea.ar2)
# calculate residuals
sea_train$resid.opt <- rep(0, ntrain)
sea_train$resid.opt <- sea_train$GMSL - sea_train$trend.opt
# find the autocorrelation within the residuals
autosea.opt <- data.frame(lag = acf(sea_train$resid.opt[3:ntrain], lag.max=10, plot = FALSE)$lag, 
                   acf = acf(sea_train$resid.opt[3:n], lag.max=10, plot = FALSE)$acf)
ci.high <- qnorm(0.975)/sqrt(ntrain)
ci.low <- - ci.high
# calculate the standard deviation of the residuals
sdres.opt <- sd(sea_train$resid.opt[3:n])
# calculate the forecasted values (k=5)
sea_valid$low.opt <- rep(0, nvalid)
sea_valid$high.opt <- rep(0, nvalid)
for (i in seq(1, nvalid, 1)) {
  sea_valid$low.opt[i] <- sea_valid$forecast.opt[i] - 1.96*sqrt(i)*sdres.opt
  sea_valid$high.opt[i] <- sea_valid$forecast.opt[i] + 1.96*sqrt(i)*sdres.opt
}
p1 <- ggplot() +
        geom_point(data = sea_train, aes(x = t, y = GMSL), 
                   size = 3, col = "black", shape = 3) +
        geom_point(data = sea_valid, aes(x = t, y = GMSL), 
                   size = 2, col = "red", shape = 16) +
        geom_line(data = sea_train, aes(x = t, y = trend.opt), 
                  size = 0.5, col = "red", linetype = "dashed") +
        geom_line(data = sea_valid, aes(x = t, y = forecast.opt), size = 1, col = "red") +
        geom_ribbon(data = sea_valid, aes(x = t, ymin = low.opt, ymax = high.opt), fill = "blue", alpha = 0.3) +
        scale_x_continuous(limits=c(0, (ntrain + nvalid)), breaks = seq(0,(ntrain + nvalid),5)) +
        scale_y_continuous(limits=c(25, 65), breaks = seq(25,65,2)) +
        labs(title = "Sea time series: optimal AR2\ntrend, forecast", x = "t", y = "GMSL") +
        JT.theme
p2 <- ggplot(data = autosea.opt, aes(x = lag, y = acf)) +
        geom_segment(aes(xend = lag, yend = 0), alpha = 0.4, size = 2) +
        geom_hline(yintercept = 0) +
        geom_hline(yintercept = ci.high, linetype = "dashed", col = "blue") +
        geom_hline(yintercept = ci.low, linetype = "dashed", col = "blue") +
        scale_x_continuous(limits=c(0, 10), breaks = seq(0,10,1)) +
        scale_y_continuous(limits=c(-1, 1), breaks = seq(-1,1,0.1)) +
        labs(title = "Sea time series: optimal AR2\nautocorrelation of residuals", x = "lag", y = "result") +
        JT.theme
grid.arrange(p1, p2, nrow = 1)
@

\begin{figure}
\includegraphics[width=1\textwidth]{PTS-seaaropt}
\caption{Optimal AR2-model of Sea time series}
\label{fig:seaaropt}
\setfloatalignment{b}
\end{figure}
\medskip

<<>>=
w <- polyroot(c(-a2, -a1, 1))
w1 <- w[1]
w2 <- w[2]
a <- matrix(c(w1, w2, w1^2, w2^2), nrow = 2, byrow = TRUE)
b <- c(0,2)
b[1:2] <- sea_train$GMSL[1:2]
A <- solve(a,b)[1]
B <- solve(a,b)[2]
ytest <- rep(0,ntrain)
for (i in seq(1, ntrain, 1)) {ytest[i] <- A*(w1^i) + B*(w2^i)}
plot(seq(1, ntrain,1), ytest)
plot(seq(1, ntrain, 1), sea_train$trend)
@

\subsection{The random walk}\index{random walk}\index{dependency!random walk}

A stationary time series requires that the observations are independent. What does a time series look like when this is not true? An example is ''persistence" in weather forecasting: if it is sunny today with a temperature of 22\degree C, the forecast for tomorrow is ''sunny with 22\degree C". 

A simple model for such a time series would be one in which the observation at time index $t$ is equal to the previous observation (at time index $(t-1)$) plus a random component.
\begin{equation}
	y_{t} = y_{t-1} + \epsilon_{t}
\label{eq:randomwalk}
\end{equation}

Of course we will have to define the starting point $y_{0}$ and the type of the random component $\epsilon$. Let's set $y_{0}=0$ and $\epsilon$ randomly taken from a standard normal distribution. Figure~\ref{fig:randomwalk1} shows 3 graphs gene\-rated with Equation~\ref{eq:randomwalk} (the random generator in \textbf{\textsf{R}} causes these graphs to differ).

<<label=randomwalk1, fig=TRUE, include=FALSE, echo=FALSE>>=
set.seed(2019)
nrw <- 3
N <- 100
walk <- data.frame(t = rep(seq(1, N), nrw), serie = rep(c(1:nrw), each = N), y = 0)
alpha <- 0.95
for (j in seq(1,nrw)) {
  for (i in seq(2, N)) {
    walk$y[(j-1)*N + i] <- alpha*walk$y[(j-1)*N + i - 1] + rnorm(1, 0, 1)
  }
}
ggplot(data = walk) + 
  geom_line(aes(x = t, y = y, col = as.factor(serie))) +
  labs(title = "3 random walks", x = "t", y = "y") +
  JT.theme +
  theme(legend.title = element_blank())
@

\begin{figure}
\includegraphics[width=0.9\textwidth]{PTS-randomwalk1}
\caption{3 different random walks}
\label{fig:randomwalk1}
\setfloatalignment{b}
\end{figure}

\newpage
\section{Independency}\index{independency}

In Figure~\ref{fig:independency1}, we see two graphs with more or less the same appearance. The graph on the left shows measurements of temperature (\degree C) and pressure (bar) of a gas (in this case 35 g (= 1.208 mole) of dry air within a cilinder with volume V=25 l). It is a standard experiment in physics checking the ideal gas law which says that temperature and pressure should be proportional. The graph on the right shows the proportion of EU-immigrants to the UK per quarter, starting in Q4 of 2009 and ending in Q2 of 2012. The reference is the starting value (Q4 of 2009).

<<label=independency1, fig=TRUE, include=FALSE, echo=FALSE>>=
#
# Ideal gas experiment: relation between pressure and temperature
#
# 35g=1.208 mol dry air in 25l volume
# temp in °C
# pressure in bar (devide pressure in Pa by 10^5)
# p = (n*R*T)/V = (1.208*8.314*(temp + 273.15))/(25*(10^-3)*(10^5)) + error term
#
set.seed(2019)
ideal <- data.frame(temp = seq(0, 180, 20), press = (1.208*8.314*(seq(0, 180, 20) + 273.15))/2500 + rnorm(10, 0, 0.025))
p1 <- ggplot(data = ideal) +
        geom_point(aes(x = temp, y = press), col = "red") +
        labs(title = "Experiment\nideal gas law", x = "temperature (°C)", y = "pressure (bar)") +
        scale_y_continuous(limits = c(0.9, 1.9)) +
        JT.theme
#
# Number of EU-immigrants to the UK from 2009 - Q4 to 2012-Q2 (2009-Q4 = 100). Source: Office for National Statistics (UK) - Provisional long-term international migration estimate
#
EUimm <- data.frame(date = seq(ymd("2009-12-01"), ymd("2012-06-01"), by = "quarters"), number = c(1, 1.06, 1.14, 1.23, 1.4, 1.35, 1.43, 1.5, 1.65, 1.77, 1.79))
p2 <- ggplot(data = EUimm) +
        geom_point(aes(x = date, y = number), col = "red") +
        labs(title = "Time series\nEU-immigrants to the UK", x = "date", y = "2009 Q4 = 1.0") +
        scale_y_continuous(limits = c(0.9, 1.9)) +
        JT.theme
grid.arrange(p1, p2, nrow = 1)
@

\begin{figure}
\includegraphics[width=0.8\textwidth]{PTS-independency1}
\caption{Independency}
\label{fig:independency1}
\setfloatalignment{b}
\end{figure}

The graphs look similar but there is a big difference. A proper scientific experiment uses the \emph{design of experiments}-methodology\index{design of experiments}\index{DOE!Design of Experiments}. One of its most important aspects is to make sure that each set of observations of temperature and pressure is made \emph{independently} of previous or future measurements. This is done by \emph{randomisation}\index{randomisation}. Instead of starting with a temperature of 0\degree C and stepswise increasing the temperature with 20\degree C until we reach 180\degree C (the \emph{standard order}\index{standard order}), we will do this in a random way (the \emph{run order}\index{run order}).

<<>>=
run.order <- sample(seq(0, 180, 20))
run.order
@

We start at 80\degree C, then we go to 100\degree C, then to 140\degree C and then we cool to 20\degree C etc. It is obvious that this requires much more time for heating, cooling, reheating etc. than when we use the standard order. However, it is worth it because in this way we eliminate the possible influence of a steady increase in temperature on the measurement (e.g. it might steadily change the volume of the gascontainer due to expansion). In short: we set up our experiment is such a way that we the observations are \emph{independent}\index{independency} of each other (Figure~\ref{fig:independency2}-left). Plotting the measurement of pressure as a function of the moment in time that it was recorded, and assuming it takes 1 hour between two consecutive measurements, we get something like Figure~\ref{fig:independency2}-middle\sidenote{In reality the measurements will probably not be nicely evenly spaced}. 

We created the \emph{run order}\index{run order} by making a random permutation of the temperatures 0\degree C, 20\degree C, ..., 180\degree C. This makes it random, but not independent. Independency is only guaranteed when we do a random pick with \emph{replacement}. This is the second essential component in the design of an experiment: \emph{replication}\index{replication}. A \emph{run order with replacement}\index{run order!with replacement} for a total of 100 experiments can be generated in \textbf{\textsf{R}} and gives Figure~\ref{fig:independency2}-right.

<<>>=
run.order.rep <- data.frame(time = seq(1, 100),
                            temp = sample(seq(0, 180, 20),
                                          size = 100,
                                          replace = TRUE))
@

<<label=independency2, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- ggplot(data = ideal) +
        geom_point(aes(x = temp, y = press), col = "red") +
        geom_point(aes(x = temp, y = (press + 0.035)), 
                   col = "black", shape = 1, size = 5) +
        geom_text(aes(x = temp, y = (press + 0.035)), 
                  label = c("5", "4", "7", "8", "1", "2", "9", "3", "6", "10"), size = 3) +
        labs(title = "Experiment\nideal gas law", x = "temperature (°C)", y = "pressure (bar)") +
        scale_y_continuous(limits = c(0.9, 1.9)) +
        JT.theme
ideal$ts <- c(ideal$press[5], ideal$press[6], ideal$press[8], ideal$press[2], ideal$press[1], ideal$press[9], ideal$press[3], ideal$press[4], ideal$press[7], ideal$press[10])
ideal$time <- seq(1,10)
p2 <- ggplot(data = ideal) +
        geom_point(aes(x = time, y = ts), col = "red") +
        geom_point(aes(x = time, y = (ts + 0.035)), 
                   col = "black", shape = 1, size = 5) +
        geom_text(aes(x = time, y = (ts + 0.035)), 
                  label = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"), size = 3) +
        labs(title = "Experiment\nrun order", x = "time (h)", y = "pressure (bar)") +
        scale_y_continuous(limits = c(0.9, 1.9)) +
        JT.theme
p3 <- ggplot(data = run.order.rep) +
        geom_point(aes(x = time, y = temp)) +
        labs(title = "Experiment\nrun order with replacement", x = "time (h)", y = "temperature (°C)") +
        scale_y_continuous(limits = c(-20, 200)) +
        JT.theme
grid.arrange(p1, p2, p3, nrow = 1)
@

\begin{figure}
\includegraphics[width=0.8\textwidth]{PTS-independency2}
\caption{Sequence of observations}
\label{fig:independency2}
\setfloatalignment{b}
\end{figure}

This looks very much like a \emph{stationary} time series and the \emph{Ljung-Box}-test confirms this.

<<>>=
Box.test(run.order.rep$temp, type = "Ljung-Box")
@
\medskip
With the time series of the EU-migrants to the UK this is not possible. We cannot design our measurements in such a way that the first measurement will be taken in June 2010 (Q2 of 2010), the next mea\-surement in December 2011 (Q4 of 2011), the third in December 2009 (Q4 of 2009) etc. Either the future is not accessible, or if we wait, the past cannot be redone. We have not choice but to make our observations starting in Q4 2009, ending in Q2 2012 and this every quarter in its proper turn (Figure~\ref{fig:independency2}-right). Time is an absolute reference point that will only occurs once. Therefore we cannot guarantee that the observations in a time series are \emph{independent}.

<<label=independency3, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = EUimm) +
      geom_point(aes(x = date, y = number), col = "red") +
      geom_point(aes(x = date, y = (number + 0.035)), 
                  col = "black", shape = 1, size = 5) +
      geom_text(aes(x = date, y = (number + 0.035)), 
                  label = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11"), size = 3) +
      labs(title = "Time series\nEU-immigrants to the UK", x = "date", y = "2009 Q4 = 1.0") +
      scale_y_continuous(limits = c(0.9, 1.9)) +
      JT.theme
@

\begin{figure}
\includegraphics[width=0.8\textwidth]{PTS-independency3}
\caption{Sequence of observations in time series}
\label{fig:independency3}
\setfloatalignment{b}
\end{figure}

For general time series it is prudent to suppose that the measurements are \emph{not independent}\index{not independent}.The uncertainty about independence has serious consequences because many standard statistical methods (e.g. linear regression) can only be applied securely on condition that the observations are independent.

\newpage
For example: Figure~\ref{fig:Amtrakdata} shows the monthly number (in thousands) of people who used the Amtrak railway system between Januari 1991 up to March 2004.

<<label=Amtrakbackground,fig=TRUE,include=FALSE, echo=FALSE>>=
Amtrak.data <- read.csv("Data/Amtrak data.csv", sep=";", stringsAsFactors = FALSE)
# transforming the ''Month" column into a proper date variable
Amtrak.data %>%  mutate(day = myd(paste0(Month, "-01"))) -> Amtrak.data
Amtrak.data$t <- c(1:nrow(Amtrak.data))
datebreaks <- seq(as.Date("1991-01-01"), as.Date("2009-06-01"), by = "2 year")
ggplot(data=Amtrak.data) +
                        geom_line(aes(x=day, y=Ridership), size=0.75, color="white", alpha = 0) +
                        scale_x_date(breaks = datebreaks, labels = date_format("%Y-%m")) +
                        expand_limits(x = as.Date(c("1990-06-01", "2007-06-01"))) +
                        labs(title="Monthly Amtrak ridership data", 
                             x = "Date",
                             caption = "Practical Time Series Forecasting with R - Shmueli and Lichtendahl Jr.") +
                        JT.theme
@

<<label=Amtrakdata,fig=TRUE,include=FALSE, echo=FALSE>>=
Amtrak.data <- read.csv("Data/Amtrak data.csv", sep=";", stringsAsFactors = FALSE)
# transforming the ''Month" column into a proper date variable
Amtrak.data %>%  mutate(day = myd(paste0(Month, "-01"))) -> Amtrak.data
Amtrak.data$t <- c(1:nrow(Amtrak.data))
datebreaks <- seq(as.Date("1991-01-01"), as.Date("2009-06-01"), by = "2 year")
Amtrak.baseplot <- ggplot(data=Amtrak.data) +
                          geom_line(aes(x=day, y=Ridership), size=0.75, color="red") +
                          scale_x_date(breaks = datebreaks, labels = date_format("%Y-%m")) +
                          expand_limits(x = as.Date(c("1990-06-01", "2007-06-01"))) +
                          labs(title="Monthly Amtrak ridership data", 
                               x = "Date",
                               caption = "Practical Time Series Forecasting with R - Shmueli and Lichtendahl Jr.") +
                          JT.theme
Amtrak.baseplot
@

\begin{marginfigure}[-1cm]
\includegraphics[width=1\textwidth]{PTS-Amtrakdata}
\caption{Amtrak: data}
\label{fig:Amtrakdata}
\setfloatalignment{b}
\end{marginfigure}

Our eyes suggest that there are patterns within these data: there is a wave-like motion around a more general curve that first goes down and then starts to rise (Figure~\ref{fig:Amtraktrendseason})

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTS-Amtraktrendseason}
\caption{Amtrak: pattern}
\label{fig:Amtraktrendseason}
\setfloatalignment{b}
\end{marginfigure}

With this pattern in mind we have a tendency to \emph{extend} this into to future and to generate something like Figure~\ref{fig:Amtrakpred}.

\begin{figure}
\includegraphics[width=1\textwidth]{PTS-Amtrakpred}
\caption{Amtrak: into the future}
\label{fig:Amtrakpred}
\setfloatalignment{b}
\end{figure}

Even when we use \emph{cross-sectional}\index{data!cross-sectional} data\sidenote[][-2cm]{cross-sectional data are collected by observing many subjects at the same point of time. E.g. the exam results of a group of students, the SAT-scores of these students at the end of their secondary education} to try to find a connection between these data (e.g. by linear regression), we implore our students to take care \emph{not} to extend models based on these data \emph{outside} the range into which they were developed. Forecasting, on the contrary, wants to do exactly this.

\newpage
\section{Appendix 1}
\label{sec:appendix1}

A classical physics experiment is investigating the distance covered by a free falling ball in Earth's gravity. The distance traveled in a time $t$ is given by:
\begin{equation}
	y(t)=\frac{g t^{2}}{2} \quad with \quad g=9.81 m/s^{2}
\end{equation}

However, nothing forces us to measure the distance $y$ for $t=0s$, then $t=1 s$, then $t=2s$ etc. We can randomise the experiment and make the first the first measurement at $t=8s$, then at $t=5s$, than $t=1s$ etc. This means that the results will be independent of each other. Essentially the question is: are we constrained to make our observations in a strict sequence.

\printindex

\newpage

\textbf{Thanks} \\
\medskip
R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
\medskip
<<>>=
sessionInfo()
@

\end{document}