\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Proper Time Series}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=PTS}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(scales)
library(tidyverse)
library(lubridate)
library(broom)
library(funModeling)
library(forecast)
library(gridExtra)
library(writexl)
library(plotly)
library(ggfortify)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions
<<echo=FALSE>>=
TRJ.FFT <- function(signal.df) {
    # This function calculates the FFT for a time-series stored in a data frame with as first 
    # column the time (or order of measurement) and as second column the vector of measurements.
    # The result is a list. 
    # The first element of the list is freqspec: the N frequencies plus for each frequency the 
    # amplitude and phase.
    # The second element of the list is resultaat: a data frame with those frequencies for which 
    # the amplitude  is at least 33% of the maximum amplitude. 
    # The data frame is sorted from highes amplitude to lowest. 
    # This data frame can be seen as containing the most influential frequencies.
    signal <- signal.df
    names(signal) <- c("t","x")
    N <- nrow(signal)
    Ts <- as.numeric(signal$t[2]-signal$t[1])
    Fs <- 1/Ts
    # Calculation of the double sided en single sided spectrum
    z <- fft(signal$x)
    P2 <- Mod(z/N)
    P1 <- P2[1:((N/2)+1)]
    P1[2:(length(P1)-1)] <- 2*P1[2:(length(P1)-1)]
    freq <- seq(0, (Fs/2)-(Fs/N), Fs/N)
    freqspec <- data.frame(freq=freq,amp=P1[1:(N/2)],arg=Arg(z[1:(N/2)]))
    # Finding the most important elements in the frequency spectrum
    grens <- ifelse(freqspec$freq[freqspec$amp==max(freqspec$amp)]==0,max(freqspec$amp[2:nrow(freqspec)])/3,max(freqspec$amp)/3)
    aantal <- length(freqspec$amp[freqspec$amp>grens])
    resultaat <- data.frame(freq=rep(0,aantal), amp=rep(0,aantal), fasehoek=rep(0,aantal))
    resultaat <- data.frame(freq=freqspec$freq[freqspec$amp>grens],
                            amp=freqspec$amp[freqspec$amp>grens],
                            fasehoek_pi=freqspec$arg[freqspec$amp>grens]/pi)
    resultaat <- resultaat[order(-resultaat$amp),]
    return(list("freqspec"=freqspec,"resultaat"=resultaat))
}
@


\frontmatter
\chapter*{Proper Time Series}

\mainmatter

\chapter{Introduction}

\section{Time series}\index{time series}

A \emph{time series} is a sequence of values of a quantity $y$ taken in a strict time order.

\newthought{Examples}
\medskip
\begin{itemize}
	\item the temperature of a blast furnace taken every second
	\item the opening share price of a company on Nasdaq (daily)
	\item the number of federal prisoners in the USA (monthly)
  \item the number of bicycles crossing an intersection within a period of one hour
	\item the amount of rainfall in a specific place within a period of one month
\end{itemize}

Figure~\ref{fig:Microsoft1} gives the graphical representation of a time series.

\begin{marginfigure}[-2cm]
\includegraphics[width=1\textwidth]{"Graphics/Microsoft1"}
\caption{}
\label{fig:Microsoft1}
\setfloatalignment{b}
\end{marginfigure}

\section{The trouble with forecasting}

Forecasting, by definition, is about the future: you are stepping into unknown territory and this brings with it Niels Bohr's warning:
\begin{quotation}
 ''It is very hard to predict, especially the future"
\end{quotation}

Because we, humans, have the privilege of vision, we have the ability to see patterns. In Figure~\ref{fig:Microsoft1} we can see that the observations of $y$ lie approximately within the blue region (Figure~\ref{fig:Microsoftextra}-left). We also have an inbuilt tendency to extrapolate. Knowing only the data given in Figure~\ref{fig:Microsoft1} we are tempted to make assumptions for what lies in the future and thus, to make a forecast (Figure~\ref{fig:Microsoftextra}-middle). Those who are very convinced of their predictive powers, could even imagine a pattern in the wiggly bits of the data, and come up with an even more detailed forecast (Figure~\ref{fig:Microsoftextra}-right).

\begin{figure*}
\includegraphics[width=1\textwidth]{"Graphics/Microsoftextra"}
\caption{Our predictive powers}
\label{fig:Microsoftextra}
\setfloatalignment{b}
\end{figure*}

\begin{marginfigure}[5cm]
\includegraphics[width=1\textwidth]{"Graphics/Microsoft5"}
\caption{}
\label{fig:Microsoft5}
\setfloatalignment{b}
\end{marginfigure}

Sadly, we would have been completely wrong. The actual evolution of this particular time series was completely different from what we expected (Figure~\ref{fig:Microsoft5}).

Figure~\ref{fig:Microsoft1} shows the time series of the value of Microsoft shares from 01-01-1998 up to 31-03-2000. Then, in April 2000, share prices dropped 15.6 percent (Figure~\ref{fig:Microsoft5}) when it announced disappointed earnings and took a massive \$900 million writedown due to unsold copies of its mobile operating system ''Surface RT". Forecasting of share prices based on their time series are notoriously difficult, but the example should show us that our ''intuition" is not a good guide for forecasting.

\section{Stationary time series}\index{stationary}\index{time series!stationary}
What should a time series look like so that we can make confident forecasts? What should the values from the past look like so that they can safely be extended to the future? They should be ''the same", which means that if we take a sample of n observations starting from time index $t_{1}$ up to time index $t_{1+n}$, and we take another sample of n observations starting from time index $t_{2}$ up to time index $t_{2+n}$ they should look \emph{the same}. This does not mean they will be indentical, but that they have the same underlying nature. Statistics gives us the vocabulary to translate these general words into something that we can check. 

\begin{itemize}
	\item We say that a time series is \emph{stationary in the mean}\index{stationary!in the mean} when $\overline{y_{t_{1}}}=\frac{\sum_{i=1}^{i=n}y_{t1}}{n} = \overline{y_{t_{2}}}=\frac{\sum_{i=1}^{i=n}y_{t2}}{n}=cnst$ for any $t_{1}$ and $t_{2}$ we choose. This is also called \emph{first order stationary}\index{stationarity!first order}
	\item A time series is \emph{stationary in the variance}\index{stationary!in the variance} when $var({y_{t_{1}}} \ldots {y_{t_{1+n}}}) = var({y_{t_{2}}} \ldots {y_{t_{2+n}}})$ for any $t_{1}$ and $t_{2}$ we choose. A time series that is stationary in the mean and in the variance is called \emph{second order stationary}\index{stationarity!second order})
	\item A time series is \emph{stationary in the $n^{th}$-order}\index{stationary!in the $n^{th}$-order} when moments up to the $n^{th}$-moment are constant.
\end{itemize}

We can easily generate a stationary time series: if the sequential observations $y_{t}$ are \emph{independent}\index{independence} and all come from an \emph{identical}\index{indentical} \emph{probability distribution}\index{IID = Independent Identical Distribution} the result will be a stationary time series. \textbf{\textsf{R}} can gererate these easily. For example we can simulate the throw of a dice: we use the \textit{sample}-function from \textbf{\textsf{R}} and, by replacing each drawn number, we make sure that each throw is independent. Another possibility to construct a stationary time series is the random draw from a normal distribution. \textbf{\textsf{R}}'s (pseudo) random number generator takes care of the independency, and we use the same normal distribution.

<<label=stationary, fig=TRUE, include=FALSE, echo=FALSE>>=
set.seed(2019)
N <- 100
stationary <- data.frame(t = seq(1,100), ydice = sample(c(1:6), N, replace = TRUE), ynorm = rnorm(N,3.5, 1))
ggplot(data = stationary, aes(x = t)) +
  geom_line(aes(y = ydice), col = "red") +
  geom_line(aes(y = ynorm), col = "blue") +
  scale_y_continuous(limits=c(-1, 7)) +
  labs(title = "Examples of stationary time series", x = "t", y = "result") +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTS-stationary}
\caption{Two stationary time series: dice and normal}
\label{fig:stationary}
\setfloatalignment{b}
\end{marginfigure}

Because in Figure~\ref{fig:stationary} the red lines switch between integer values, we can conclude that they come from the dice experiment. But both graphs have a very irregular look. And it might come as a surprise that we consider these \emph{stationary} time series excellent material for forecasting! On the contrary: it seems that their behaviour is our worst nightmare when we intend to predict what will happen after time index 100.

However, the situation is really not that bad! Often it is OK to assume that these stationary time series come from an normal distribution. As estimate for the mean we take the sample mean, as estimate for the variance, the sample variance. Using these two we can calculate the $\alpha$ confidence interval. For $\alpha=95\%$ we get Figure~\ref{fig:stationaryCI}.

<<label=stationaryCI, fig=TRUE, include=FALSE, echo=FALSE>>=
mu.dice <- mean(stationary$ydice)
sd.dice <- sd(stationary$ydice)
low.lim.dice <- mu.dice - 1.96*sd.dice
high.lim.dice <- mu.dice + 1.96*sd.dice
mu.norm <- mean(stationary$ynorm)
sd.norm <- sd(stationary$ynorm)
low.lim.norm <- mu.norm - 1.96*sd.norm
high.lim.norm <- mu.norm + 1.96*sd.norm
N.ext <- 25
stationary.ext <- data.frame(t = seq((N + 1),(N + N.ext)), ydice = sample(c(1:6), N.ext, replace = TRUE), ynorm = rnorm(N.ext,3.5, 1))
ggplot() +
  geom_line(data = stationary, aes(x = t, y = ydice), col = "red") +
  geom_line(data = stationary, aes(x = t, y = ynorm), col = "blue") +
  geom_line(data = stationary.ext, aes(x = t, y = ydice), col = "red", lty = 2) +
  geom_line(data = stationary.ext, aes(x = t, y = ynorm), col = "blue", lty = 2) +
  labs(title = "Confidence intervals for prediction of stationary time series", x = "t", y = "result") +
  geom_hline(yintercept = low.lim.dice, col = "red") +
  geom_hline(yintercept = high.lim.dice, col = "red") +
  geom_hline(yintercept = low.lim.norm, col = "blue") +
  geom_hline(yintercept = high.lim.norm, col = "blue") +
  scale_y_continuous(limits=c(-1, 7)) +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTS-stationaryCI}
\caption{Confidence intervals of forecasted values}
\label{fig:stationaryCI}
\setfloatalignment{b}
\end{marginfigure}

As we can see, the new values ($t>100$) fall (almost) within the predicted confidence intervals (red for the dice experiment, blue for the normal sampling experiment). And while it is true that this does not give us accurate predictions on an experiment-by-experiment basis, it gives a general idea of what we can expect. At this moment we can assure the reader that better prediction methods are available for stationary time series!

\newpage
For example: Figure~\ref{fig:Amtrakdata} shows the monthly number (in thousands) of people who used the Amtrak railway system between Januari 1991 up to March 2004.

<<label=Amtrakbackground,fig=TRUE,include=FALSE, echo=FALSE>>=
Amtrak.data <- read.csv("Data/Amtrak data.csv", sep=";", stringsAsFactors = FALSE)
# transforming the ''Month" column into a proper date variable
Amtrak.data %>%  mutate(day = myd(paste0(Month, "-01"))) -> Amtrak.data
Amtrak.data$t <- c(1:nrow(Amtrak.data))
datebreaks <- seq(as.Date("1991-01-01"), as.Date("2009-06-01"), by = "2 year")
ggplot(data=Amtrak.data) +
                        geom_line(aes(x=day, y=Ridership), size=0.75, color="white", alpha = 0) +
                        scale_x_date(breaks = datebreaks, labels = date_format("%Y-%m")) +
                        expand_limits(x = as.Date(c("1990-06-01", "2007-06-01"))) +
                        labs(title="Monthly Amtrak ridership data", 
                             x = "Date",
                             caption = "Practical Time Series Forecasting with R - Shmueli and Lichtendahl Jr.") +
                        JT.theme
@

<<label=Amtrakdata,fig=TRUE,include=FALSE, echo=FALSE>>=
Amtrak.data <- read.csv("Data/Amtrak data.csv", sep=";", stringsAsFactors = FALSE)
# transforming the ''Month" column into a proper date variable
Amtrak.data %>%  mutate(day = myd(paste0(Month, "-01"))) -> Amtrak.data
Amtrak.data$t <- c(1:nrow(Amtrak.data))
datebreaks <- seq(as.Date("1991-01-01"), as.Date("2009-06-01"), by = "2 year")
Amtrak.baseplot <- ggplot(data=Amtrak.data) +
                          geom_line(aes(x=day, y=Ridership), size=0.75, color="red") +
                          scale_x_date(breaks = datebreaks, labels = date_format("%Y-%m")) +
                          expand_limits(x = as.Date(c("1990-06-01", "2007-06-01"))) +
                          labs(title="Monthly Amtrak ridership data", 
                               x = "Date",
                               caption = "Practical Time Series Forecasting with R - Shmueli and Lichtendahl Jr.") +
                          JT.theme
Amtrak.baseplot
@

\begin{marginfigure}[-1cm]
\includegraphics[width=1\textwidth]{PTS-Amtrakdata}
\caption{Amtrak: data}
\label{fig:Amtrakdata}
\setfloatalignment{b}
\end{marginfigure}

Our eyes suggest that there are patterns within these data: there is a wave-like motion around a more general curve that first goes down and then starts to rise (Figure~\ref{fig:Amtraktrendseason})

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTS-Amtraktrendseason}
\caption{Amtrak: pattern}
\label{fig:Amtraktrendseason}
\setfloatalignment{b}
\end{marginfigure}

With this pattern in mind we have a tendency to \emph{extend} this into to future and to generate something like Figure~\ref{fig:Amtrakpred}.

\begin{figure}
\includegraphics[width=1\textwidth]{PTS-Amtrakpred}
\caption{Amtrak: into the future}
\label{fig:Amtrakpred}
\setfloatalignment{b}
\end{figure}



Even when we use \emph{cross-sectional}\index{data!cross-sectional} data\sidenote[][-2cm]{cross-sectional data are collected by observing many subjects at the same point of time. E.g. the exam results of a group of students, the SAT-scores of these students at the end of their secondary education} to try to find a connection between these data (e.g. by linear regression), we implore our students to take care \emph{not} to extend models based on these data \emph{outside} the range into which they were developed. Forecasting, on the contrary, wants to do exactly this.

\printindex

\newpage

\textbf{Thanks} \\
\medskip
R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
\medskip
<<>>=
sessionInfo()
@

\end{document}