\documentclass{tufte-book}
\usepackage{graphicx}  % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{url}
\usepackage{lipsum}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
% Prints a trailing space in a smart way.
\usepackage{xspace}
\usepackage{hyperref}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\newcommand{\tthdump}[1]{#1}

\newcommand{\openepigraph}[2]{
  \begin{fullwidth}
  \sffamily\large
    \begin{doublespace}
      \noindent\allcaps{#1}\\ % epigraph
      \noindent\allcaps{#2} % author
    \end{doublespace}
  \end{fullwidth}
}


\usepackage{makeidx}
\makeindex

\title{Proper Time Series}
\author{Jan Trommelmans}

\begin{document}
\SweaveOpts{concordance=TRUE,prefix.string=PTS}
\setkeys{Gin}{width=1.1\marginparwidth} %% Sweave

<<echo=FALSE>>=
library(scales)
library(tidyverse)
library(lubridate)
library(broom)
library(funModeling)
library(forecast)
library(gridExtra)
library(writexl)
library(plotly)
library(ggfortify)
library(latex2exp)
library(tseries)
@

% Setting the ggplot theme:
<<echo=FALSE>>=
JT.theme <- theme(panel.border = element_rect(fill = NA, colour = "gray10"),
                  panel.background = element_blank(),
                  panel.grid.major = element_line(colour = "gray85"),
                  panel.grid.minor = element_line(colour = "gray85"),
                  panel.grid.major.x = element_line(colour = "gray85"),
                  axis.text = element_text(size = 8 , face = "bold"),
                  axis.title = element_text(size = 9 , face = "bold"),
                  plot.title = element_text(size = 12 , face = "bold"),
                  strip.text = element_text(size = 8 , face = "bold"),
                  strip.background = element_rect(colour = "black"),
                  legend.text = element_text(size = 8),
                  legend.title = element_text(size = 9 , face = "bold"),
                  legend.background = element_rect(fill = "white"),
                  legend.key = element_rect(fill = "white"))
@

% Functions
<<echo=FALSE>>=
TRJ.FFT <- function(signal.df) {
    # This function calculates the FFT for a time-series stored in a data frame with as first 
    # column the time (or order of measurement) and as second column the vector of measurements.
    # The result is a list. 
    # The first element of the list is freqspec: the N frequencies plus for each frequency the 
    # amplitude and phase.
    # The second element of the list is resultaat: a data frame with those frequencies for which 
    # the amplitude  is at least 33% of the maximum amplitude. 
    # The data frame is sorted from highes amplitude to lowest. 
    # This data frame can be seen as containing the most influential frequencies.
    signal <- signal.df
    names(signal) <- c("t","x")
    N <- nrow(signal)
    Ts <- as.numeric(signal$t[2]-signal$t[1])
    Fs <- 1/Ts
    # Calculation of the double sided en single sided spectrum
    z <- fft(signal$x)
    P2 <- Mod(z/N)
    P1 <- P2[1:((N/2)+1)]
    P1[2:(length(P1)-1)] <- 2*P1[2:(length(P1)-1)]
    freq <- seq(0, (Fs/2)-(Fs/N), Fs/N)
    freqspec <- data.frame(freq=freq,amp=P1[1:(N/2)],arg=Arg(z[1:(N/2)]))
    # Finding the most important elements in the frequency spectrum
    grens <- ifelse(freqspec$freq[freqspec$amp==max(freqspec$amp)]==0,max(freqspec$amp[2:nrow(freqspec)])/3,max(freqspec$amp)/3)
    aantal <- length(freqspec$amp[freqspec$amp>grens])
    resultaat <- data.frame(freq=rep(0,aantal), amp=rep(0,aantal), fasehoek=rep(0,aantal))
    resultaat <- data.frame(freq=freqspec$freq[freqspec$amp>grens],
                            amp=freqspec$amp[freqspec$amp>grens],
                            fasehoek_pi=freqspec$arg[freqspec$amp>grens]/pi)
    resultaat <- resultaat[order(-resultaat$amp),]
    return(list("freqspec"=freqspec,"resultaat"=resultaat))
}
@


\frontmatter
\chapter*{Proper Time Series}

\mainmatter

\chapter{Introduction}

\section{Time series}\index{time series}

A \emph{time series} is a sequence of values of a quantity $y$ taken at consecutive points in time, in a  strict order. Often the points in time are absolute: Monday March 18th 2019, Tuesday March 19th 2019, etc. The interval between these points in time is usually constant. It can be nanoseconds or millenia. With each point in time we can associate a time index\index{time index} going from $1$ for the first point in time, to $n$ for the last point in time of our data series.

<<label=startgrafiek, fig=TRUE, include=FALSE, echo=FALSE>>=
# Number of elements in the data set
  N <- 1000
# Random number generator seed
  set.seed(2019)
# error parameters
  mu <- 0
  sigma <- 3
# trend model parameters
  b0 <- 0
  b1 <- 0.01
  b2 <- 0.00005
# seasonality 1
  amp1 <- 3*sigma
  teta1 <- 0
  T1 <- N/9
# seasonality 2
  amp2 <- sigma
  teta2 <- pi/3
  T2 <- N/39
# constant for combined additive/multiplicative time-series
  c <- 0.05
#
# setting up the different components
#
  construct <- data.frame(t = seq(1:N),
                            error = rnorm(N, mean = mu, sd = sigma))
  construct %>% mutate(trend = (b0 + b1*t +b2*t^2)) -> construct
  construct %>% mutate(season1 = amp1*sin(2*pi*t/T1 + teta1)) -> construct
  construct %>% mutate(season2 = amp2*sin(2*pi*t/T2 + teta2)) -> construct
#
# contructing the additive time series
construct %>% mutate(add = error + trend + season1 + season2) -> construct
ggplot(data = construct, aes(x = t)) +
    geom_line(aes(y = add), size = 0.35, color="red") +
    scale_y_continuous(limits= c(-25, 75)) +
    labs(y="y") +
    JT.theme
@
  
\begin{marginfigure}[-2cm]
\includegraphics[width=1\textwidth]{PTS-startgrafiek}
\caption{}
\label{fig:startgrafiek}
\setfloatalignment{b}
\end{marginfigure}

\newthought{Examples}
\medskip
\begin{itemize}
	\item the temperature of a blast furnace taken every second
	\item the opening share price of a company on Nasdaq (daily)
	\item the number of federal prisoners in the USA (monthly)
  \item the number of bicycles crossing an intersection within a period of one hour
	\item the amount of rainfall in a specific place within a period of one month
\end{itemize}

The quantity $y$ that we measure at index point $t$, we denote by $y_{t}$. Usually $y_{t}$ will depend in some measure on chance: Boeing's share price can suddenly plummet when one of its airplanes has an accident, a temperature measurement can be somewhat lower because someone left open a door, ... Therefore we consider the measurement $y$ to be one of many possible values of a \emph{chance variable} $Y$. This is true for the value $y_{t}$ at every point in time $t$: it is the value taken by the chance variable $Y_{t}$ at time $t$.

\begin{marginfigure}[-5cm]
\includegraphics[width=1\textwidth]{"PTS-startgrafiekchance"}
\caption{}
\label{fig:startgrafiekchance}
\setfloatalignment{b}
\end{marginfigure}

In Figure~\ref{fig:startgrafiekchance} we can see that the red graph is only one of many (infinite) possible graphs. We have included 4 alternatives (grey), and a detail for $t=500$ shows some representations of $Y_{500}$.


When we treat $Y_{t}$ as a chance variable, it follows that it has a probability distribution, and the typical parameters such as a mean ($\mu_{t}$), a standard deviation ($\sigma_{t}$), etc. These parameters can change so that $\mu_{1} \neq \mu_{2}$ and $\sigma_{1} \neq \sigma_{2}$. In general we can say that:
\begin{equation}
	\mu_{t}=\mu_{t}(t) \quad and \quad \sigma_{t}=\sigma_{t}(t)
\end{equation}

\section{The time series as a sample}

For most time series we do not have access to all possible values that $Y_{t}$ can take. We usually have only one value $y_{t}$ at each time point $t$. As an estimate of the mean at time point $t$ we can use the observed value $y_{t}$, but this is a very crude estimation. 

In some time series we could have the, visual, impression that our observations $y_{t}$ seem to fluctuate around a constant value. In these conditions we can make the assumption that the mean does not change with $t$. A time series with this behaviour is called \emph{stationary in the mean}\index{stationary!in the mean}, and we can estimate this constant value of $\mu$ from the sample mean:
\begin{equation}
	\bar{y} = \frac{1}{n}\sum_{i=1}^{i=n}y_{i}
\end{equation}

The same argument can be used for the \emph{variance} of $Y_{t}$. If our sample consists of one value $y_{t}$ for every time point $t$ we cannot calculate $var(Y_{t})$.

In some time series we can simplify the situation by assuming that the population variance $\sigma^{2}$ is a constant, and that we can estimate it from the sample variance:
\begin{equation}
	var(y) = \frac{1}{n-1}\sum_{i=1}^{i=n-1}\left( y_{i} - \bar{y} \right)^{2}
\end{equation}

A time series where the variance is a constant is called \emph{stationary in the variance}\index{stationary in the variance}\index{stationary!in the variance}.

A time series that is \emph{stationary in the mean} and \emph{stationary in the variance} is called \emph{second order stationary}\index{second order stationarity}\index{stationary!second order}. The idea of \emph{stationarity} will play an important role when we want to use the time series to make prediction, to make a forecast.

\section{The trouble with forecasting}

Forecasting\index{forecast}, by definition, is about the future: you are stepping into unknown territory and this brings with it Niels Bohr's warning:
\begin{quotation}
 ''It is very hard to predict, especially the future"
\end{quotation}

Because we, humans, have the privilege of vision, we have the ability to see patterns. In Figure~\ref{fig:Microsoftextra}-left we can see that the observations of $y$ lie approximately within the blue region.  We also have an inbuilt tendency to extrapolate. Knowing only the data given in Figure~\ref{fig:Microsoftextra}-left we are tempted to make assumptions for what lies in the future and thus, to make a forecast (Figure~\ref{fig:Microsoftextra}-middle). Those who are very convinced of their predictive powers, could even imagine a pattern in the wiggly bits of the data, and come up with an even more detailed forecast (Figure~\ref{fig:Microsoftextra}-right).

\begin{figure*}
\includegraphics[width=1\textwidth]{"Graphics/Microsoftextra"}
\caption{Our predictive powers}
\label{fig:Microsoftextra}
\setfloatalignment{b}
\end{figure*}

Sadly, we would have been completely wrong. The actual evolution of this particular time series was completely different from what we expected (Figure~\ref{fig:Microsoft5}).

\begin{marginfigure}[0cm]
\includegraphics[width=1\textwidth]{"Graphics/Microsoft5"}
\caption{}
\label{fig:Microsoft5}
\setfloatalignment{b}
\end{marginfigure}

Figure~\ref{fig:Microsoftextra} shows the time series of the value of Microsoft shares from 01-01-1998 up to 31-03-2000. Then, in April 2000, share prices dropped 15.6 percent (Figure~\ref{fig:Microsoft5}) when it announced disappointing earnings and took a massive \$900 million writedown due to unsold copies of its mobile operating system ''Surface RT". Forecasting of share prices based on their time series are notoriously difficult, and the example should show us that our ''intuition" is not a good guide for forecasting.

\section{The ideal time series for forecasting}\index{stationary}\index{time series!stationary}
\label{sec:ideal}
What should a time series look like so that we can make confident forecasts? What should the values from the past look like so that they can \emph{safely be extended to the future}? They should be ''the same", which means that if we take a sample of N observations starting from time index $t_{1}$ up to time index $t_{1+N}$, and we take another sample of N observations starting from time index $t_{2}$ up to time index $t_{2+N}$ they should look \emph{the same}. This does not mean they will be indentical, but that they have the same underlying nature. The underlying structure can be represented by statistical properties:
\begin{itemize}
	\item the time series should be \emph{stationary in the mean}. In that case the sample mean of the time series from time index $t_{1}$ up to time index $t_{1+N}$ will be (more or less) the same as the sample mean of the time series from time index $t_{2}$ up to time index $t_{2+N}$
	\item the time series should be \emph{stationary in the variance}. In that case the sample variance of the time series from time index $t_{1}$ up to time index $t_{1+N}$ will be (more or less) the same as the sample variance of the time series from time index $t_{2}$ up to time index $t_{2+N}$
	\item or more general: the time series should be \emph{second order stationary}
\end{itemize}

We can easily generate a stationary time series: if the sequential observations $y_{t}$ are \emph{Independent}\index{independence} and come from an \emph{Identical}\index{identical} \emph{probability Distribution} (abbreviated as \emph{IID}) \index{IID = Independent Identical Distribution} the result will be a stationary time series. For example we can simulate the throw of a dice: we use the \textit{sample}-function from \textbf{\textsf{R}} and, by replacing each drawn number, we make sure that each throw is independent (Figure~\ref{fig:stationarydice1}).

<<label=stationarydice1, fig=TRUE, include=FALSE, echo=FALSE>>=
N <- 100
stationary <- data.frame(t = seq(1,N), 
                              ydice1 = sample(c(1:6), N, replace = TRUE), 
                              ydice2 = sample(c(1:6), N, replace = TRUE), 
                              ydice3 = sample(c(1:6), N, replace = TRUE), 
                              ydice4 = sample(c(1:6), N, replace = TRUE), 
                              ydice5 = sample(c(1:6), N, replace = TRUE),
                              ynorm1 = rnorm(N, 3.5, 1),
                              ynorm2 = rnorm(N, 3.5, 1),
                              ynorm3 = rnorm(N, 3.5, 1),
                              ynorm4 = rnorm(N, 3.5, 1),
                              ynorm5 = rnorm(N, 3.5, 1),
                         mean.dice = 0,
                         sd.dice = 0,
                         mean.norm = 0,
                         sd.norm = 0)
stationary$mean.dice <- rowMeans(stationary[ , 2:6])
stationary$sd.dice <- apply(stationary[,2:6], 1, FUN=sd)
stationary$mean.norm <- rowMeans(stationary[ , 7:11])
stationary$sd.norm <- apply(stationary[,7:11], 1, FUN=sd)
ggplot(data = stationary, aes(x = t)) +
  geom_line(aes(y = ydice1), col = "red") +
  scale_y_continuous(limits=c(-1, 7)) +
  labs(title = "Example of stationary time series\n(uniform distribution)", x = "t", y = "result") +
  JT.theme
@

\begin{marginfigure}[-14cm]
\includegraphics[width=1\textwidth]{PTS-stationarydice1}
\caption{Stationary time series: $y_{t}$}
\label{fig:stationarydice1}
\setfloatalignment{b}
\end{marginfigure}

The advantage of constructing your own time series, is that you can redo it! Figure~\ref{fig:stationarydice2} shows 4 other alternatives (in grey) which gives us a flavor of the possibilities of the chance variable $Y_{t}$.

<<label=stationarydice2, fig=TRUE, include=FALSE, echo=FALSE>>=
color <- c('chartreuse3', 'cornflowerblue', 'darkgoldenrod1', 'peachpuff3','mediumorchid2')
ggplot(data = stationary, aes(x = t)) +
  geom_line(aes(y = ydice1), col = color[1]) +
  geom_line(aes(y = ydice2), col = color[2]) +
  geom_line(aes(y = ydice3), col = color[3]) +
  geom_line(aes(y = ydice4), col = color[4]) +
  geom_line(aes(y = ydice5), col = color[5]) +
  scale_y_continuous(limits=c(0, 7)) +
  labs(title = "Example of stationary time series\n(uniform distribution)", x = "t", y = "result") +
  JT.theme
@

\begin{marginfigure}[-5cm]
\includegraphics[width=1\textwidth]{PTS-stationarydice2}
\caption{Stationary time series: $Y_{t}$}
\label{fig:stationarydice2}
\setfloatalignment{b}
\end{marginfigure}

These 5 time series are clearly not identical. At time point $t=50$ we have the following values for $y_{50}$:
<<>>=
stationary[50, 2:6]
@

But when we calculate the mean value (red) and the standard deviation (blue) of $y_{t}$ at each time point $t$ we get Figure~\ref{fig:stationarydice3}:

<<label=stationarydice3, fig=TRUE, include=FALSE, echo=FALSE>>=
color <- c('chartreuse3', 'cornflowerblue', 'darkgoldenrod1', 'peachpuff3','mediumorchid2')
ggplot(data = stationary, aes(x = t)) +
  geom_line(aes(y = ydice1), col = color[1]) +
  geom_line(aes(y = ydice2), col = color[2]) +
  geom_line(aes(y = ydice3), col = color[3]) +
  geom_line(aes(y = ydice4), col = color[4]) +
  geom_line(aes(y = ydice5), col = color[5]) +
  geom_line(aes(y = mean.dice), col = "red", size = 1.5) +
  geom_line(aes(y = sd.dice), col = "blue", size = 1.5) +
  scale_y_continuous(limits=c(0, 7)) +
  labs(title = "Example of stationary time series\n(uniform distribution)", x = "t", y = "result") +
  JT.theme
@

\begin{marginfigure}[-2cm]
\includegraphics[width=1\textwidth]{PTS-stationarydice3}
\caption{Stationary time series: $Y_{t}$ with mean and standard deviation}
\label{fig:stationarydice3}
\setfloatalignment{b}
\end{marginfigure}

The mean and the standard deviation are not constant, but that is to be expected with only 5 values for $Y_{t}$ at every time point $t$.

Another example: we construct a stationary time series by taking the random draw from a normal distribution. \textbf{\textsf{R}}'s (pseudo) random number generator takes care of the independency, and we use the same normal distribution. Again, the mean and standard deviation calculated at each time point $t$ is not constant, but with 5 values in each sample, this is to be expected.

<<label=stationarynorm, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = stationary, aes(x = t)) +
  geom_line(aes(y = ynorm1), col = color[1]) +
  geom_line(aes(y = ynorm2), col = color[2]) +
  geom_line(aes(y = ynorm3), col = color[3]) +
  geom_line(aes(y = ynorm4), col = color[4]) +
  geom_line(aes(y = ynorm5), col = color[5]) +
  geom_line(aes(y = mean.norm), col = "red", size = 1.5) +
  geom_line(aes(y = sd.norm), col = "blue", size = 1.5) +
  scale_y_continuous(limits=c(0, 7)) +
  labs(title = "Example of stationary time series\n(normal distribution)", x = "t", y = "result") +
  JT.theme
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTS-stationarynorm}
\caption{Stationary time series: $Y_{t}$ with mean and standard deviation}
\label{fig:stationarynorm}
\setfloatalignment{b}
\end{marginfigure}


\section{Is my time series stationary?}
In \ref{sec:ideal} we constructed two stationary time series using the \emph{IID-principle}: \emph{Independent} samples taken from an \emph{Identical} \emph{Distribution}. 

When we are given a time series, a first sign of possible \emph{stationarity} would be that the elements of our time series are independent of one another. Or conversely: if we were to detect that the elements of our time series are not independent, it would make it very unlikely that this time series would be \emph{stationary}. 

Independence can be checked using the concepts of \emph{covariance} and \emph{correlation}. 

\subsection{Covariance and Correlation}\index{covariance}\index{correlation}
In general, when we have two random variables $A$ and $B$ the \emph{covariance} of these two is given by:
\begin{equation}
	cov(A,B)= E\left[ (A - \mu_{A})(B - \mu_{B}) \right] 
\end{equation}

Most often we do not have access to all possible values of $A$ and $B$, but we only have a set of $n$ values ($a_{1} \ldots a_{n}$) and $n$ values ($b_{1} \ldots b_{n}$) (a sample from $A$ and a sample from $B$). These samples form $n$ pairs $(a_{i}, b_{i}) \quad i=1 \ldots n$. We can estimate the population means $\mu_{A}$ and $\mu_{B}$ from
\begin{equation}
	\bar{a} = \frac{1}{n}\sum_{i=1}^{i=n}a_{i} \quad and \quad \bar{b} = \frac{1}{n}\sum_{i=1}^{i=n}b_{i}
\end{equation}

and calculate the \emph{sample covariance}\index{sample covariance}\index{covariance!of sample}:
\begin{equation}
	Cov(a,b)= \frac{1}{n-1}\sum_{i=1}^{i=n}(a_{i}-\bar{a})(b_{i}-\bar{b})
\end{equation}

From this definition we can see that if we calculate the sample covariance of n pairs $(a_{i}, b_{i})$ and of n pairs $(c_{i}, d_{i})$, the result will be larger, in absolute value, for the pairs which have a components with a lot of variance. Therefore it is better to use a dimensionless measure by normalising, the \emph{sample correlation}
\begin{equation}
	Cor(a,b)=\frac{Cov(a,b)}{sd(a)sd(b)}
\end{equation}

where
\begin{equation}
	sd(a)=\sqrt{var(a)} \quad and \quad sd(b)=\sqrt{var(b)}
\end{equation}

$Cor(a,b)$ is dimensionless and its value will lie between -1 and 1. High absolute values of $Cor(a,b)$ indicate a strong correlation, values around 0 indicate no correlation at all.

\subsection{Using covariance and correlation in time series}

In time series we usually have only one set of $n$ values $y_{t}$, one for each time point $t$. Suppose the values of this times series were given by a \emph{global} formula
\begin{equation}
	y_{t} = 5 + 0.2t
\end{equation}

For time points $t=1 \ldots 5$ the corresponding values of $y$ would be $(5.2, 5.4, 5.6, 5.8, 6.0)$. Another way to construct the time series would be:
\begin{equation}
	y_{t}=y_{t-1} + 0.2 \quad with \quad y_{1}=5.2
\end{equation}
This is a \emph{local} definition that tells us that the value $y_{t}$ at time point $t$ is related to value at the previous time point. The time series is correlated with itself. There is \emph{autocorrelation}.

If we want to calculate \emph{covariance} and \emph{correlation} we need pairs $(a_{i}, b_{i})$. Comparing each point with it's previous point we get $(n-1)=4$ pairs: $(y_{2},y_{1}), (y_{3},y_{2}), (y_{4},y_{3}), (y_{5},y_{4})$. This we call \emph{auto-correlation Lag1}. But we can also compare each point with the data point two time points before. We get $(n-2)=3$ pairs: $(y_{3},y_{1}), (y_{4},y_{2}), (y_{5},y_{3})$ (\emph{autocorrelation Lag2}). The \emph{sample autocovariance function Lagk} is defined by\sidenote{for all lags we still devide by the total length $n$ of the time series, and not by the number of pairs $(n-k)$ that are used for comparison. We also use everywhere $\bar{y}$, the average of the complete time series, and not the average of the partial time series. If the time series is long ($n$ large), this does not influence the result greatly. For small values of $n$ the effect can be substantial.}:
\begin{equation}
	c_{k}=\frac{1}{n}\sum_{i=1}^{i=n-k}(y_{i+k} - \bar{y})(y_{i} - \bar{y})
	\label{eq:autocovariance}
\end{equation}

and the \emph{sample autocorrelation function Lagk} is defined by:
\begin{equation}
	r_{k}=\frac{c_{k}}{c_{0}}
\end{equation}

From this definition it is clear that $r_{0}=1$, which is what we expected: every time series is perfectly correlated with itself at Lag0. The \textbf{\textsf{R}}-function \textit{acf} gives us a graph of the autocorrelation in a time series for different lags. For our example where $y_{t}=5+0.2t$ ($n=1000$) we see in Figure~\ref{fig:autocorlin} perfect autocorrelation for all lags:

<<label=autocorlin, fig=TRUE, include=FALSE, echo=FALSE>>=
n <- 1000
autocorlin <- data.frame(t = seq(1,n), y = 0)
autocorlin$y <- 5 + 0.2*autocorlin$t
acf(autocorlin$y, lag.max=10)
@

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTS-autocorlin}
\caption{Autocorrelation: linear dependency}
\label{fig:autocorlin}
\setfloatalignment{b}
\end{marginfigure}

If however we construct the time series based on IID-prinicples (e.g. $y$ is chosen at random from a standard normal distribution) we see no autocorrelation at all (Figure~\ref{fig:autocornorm}).

<<label=autocornorm, fig=TRUE, include=FALSE, echo=FALSE>>=
n <- 1000
autocornorm <- data.frame(t = seq(1,n), y = rnorm(n, 0, 1))
acf(autocornorm$y, lag.max=10)
@

\begin{marginfigure}[5cm]
\includegraphics[width=1\textwidth]{PTS-autocornorm}
\caption{Autocorrelation: IID-constructed}
\label{fig:autocornorm}
\setfloatalignment{b}
\end{marginfigure}

There are different tests to check if a time series is stationary: the \emph{Ljung-Box}-test, the \emph{Augmented Dickey–Fuller (ADF)}-test, the \emph{Kwiatkowski-Phillips-Schmidt-Shin (KPSS)}-test.

Because in Figure~\ref{fig:stationary} the red lines switch between integer values, we can conclude that they come from the dice experiment. But both graphs have a very irregular look. And it might come as a surprise that we consider these \emph{stationary} time series excellent material for forecasting! On the contrary: it seems that their behaviour is our worst nightmare when we intend to predict what will happen after time index 100.

However, the situation is really not that bad! Often it is OK to assume that these stationary time series come from an normal distribution. As estimate for the mean we take the sample mean, as estimate for the variance, the sample variance. Using these two we can calculate the $\alpha$ confidence interval. For $\alpha=95\%$ we get Figure~\ref{fig:stationaryCI}-left.

As we can see, the new values ($t>100$) fall (almost) within the predicted confidence intervals (red for the dice experiment, blue for the normal sampling experiment). And while it is true that this does not give us accurate predictions on an experiment-by-experiment basis, it gives a general idea of what we can expect. 

We could of course also fit a linear model to these time series:

We can see that the p-values for the slope  are well above 0.05, so that there is no reason to reject the null-hypothesis that the slope = 0. This means that in both cases the predicted value in the future will be equal to the intercept value of the models  and . We can calculate a 95\% confidence interval for this model (Figure~\ref{fig:stationaryCI}-right).



\section{Dependency}

\subsection{The random walk}\index{random walk}\index{dependency!random walk}

A stationary time series requires that the observations are independent. What does a time series look like when this is not true? An example is ''persistence" in weather forecasting: if it is sunny today with a temperature of 22\degree C, the forecast for tomorrow is ''sunny with 22\degree C". 

A simple model for such a time series would be one in which the observation at time index $t$ is equal to the previous observation (at time index $(t-1)$) plus a random component.
\begin{equation}
	y_{t} = y_{t-1} + \epsilon_{t}
\label{eq:randomwalk}
\end{equation}

Of course we will have to define the starting point $y_{0}$ and the type of the random component $\epsilon$. Let's set $y_{0}=0$ and $\epsilon$ randomly taken from a standard normal distribution. Figure~\ref{fig:randomwalk1} shows 3 graphs gene\-rated with Equation~\ref{eq:randomwalk} (the random generator in \textbf{\textsf{R}} causes these graphs to differ).

<<label=randomwalk1, fig=TRUE, include=FALSE, echo=FALSE>>=
set.seed(2019)
n <- 3
N <- 100
walk <- data.frame(t = rep(seq(1, N), n), serie = rep(c(1:n), each = N), y = 0)
alpha <- 0.95
for (j in seq(1,n)) {
  for (i in seq(2, N)) {
    walk$y[(j-1)*N + i] <- alpha*walk$y[(j-1)*N + i - 1] + rnorm(1, 0, 1)
  }
}
ggplot(data = walk) + 
  geom_line(aes(x = t, y = y, col = as.factor(serie))) +
  labs(title = "3 random walks", x = "t", y = "y") +
  JT.theme +
  theme(legend.title = element_blank())
@

\begin{figure}
\includegraphics[width=0.9\textwidth]{PTS-randomwalk1}
\caption{3 different random walks}
\label{fig:randomwalk1}
\setfloatalignment{b}
\end{figure}

\newpage
\section{Independency}\index{independency}

In Figure~\ref{fig:independency1}, we see two graphs with more or less the same appearance. The graph on the left shows measurements of temperature (\degree C) and pressure (bar) of a gas (in this case 35 g (= 1.208 mole) of dry air within a cilinder with volume V=25 l). It is a standard experiment in physics checking the ideal gas law which says that temperature and pressure should be proportional. The graph on the right shows the proportion of EU-immigrants to the UK per quarter, starting in Q4 of 2009 and ending in Q2 of 2012. The reference is the starting value (Q4 of 2009).

<<label=independency1, fig=TRUE, include=FALSE, echo=FALSE>>=
#
# Ideal gas experiment: relation between pressure and temperature
#
# 35g=1.208 mol dry air in 25l volume
# temp in °C
# pressure in bar (devide pressure in Pa by 10^5)
# p = (n*R*T)/V = (1.208*8.314*(temp + 273.15))/(25*(10^-3)*(10^5)) + error term
#
set.seed(2019)
ideal <- data.frame(temp = seq(0, 180, 20), press = (1.208*8.314*(seq(0, 180, 20) + 273.15))/2500 + rnorm(10, 0, 0.025))
p1 <- ggplot(data = ideal) +
        geom_point(aes(x = temp, y = press), col = "red") +
        labs(title = "Experiment\nideal gas law", x = "temperature (°C)", y = "pressure (bar)") +
        scale_y_continuous(limits = c(0.9, 1.9)) +
        JT.theme
#
# Number of EU-immigrants to the UK from 2009 - Q4 to 2012-Q2 (2009-Q4 = 100). Source: Office for National Statistics (UK) - Provisional long-term international migration estimate
#
EUimm <- data.frame(date = seq(ymd("2009-12-01"), ymd("2012-06-01"), by = "quarters"), number = c(1, 1.06, 1.14, 1.23, 1.4, 1.35, 1.43, 1.5, 1.65, 1.77, 1.79))
p2 <- ggplot(data = EUimm) +
        geom_point(aes(x = date, y = number), col = "red") +
        labs(title = "Time series\nEU-immigrants to the UK", x = "date", y = "2009 Q4 = 1.0") +
        scale_y_continuous(limits = c(0.9, 1.9)) +
        JT.theme
grid.arrange(p1, p2, nrow = 1)
@

\begin{figure}
\includegraphics[width=0.8\textwidth]{PTS-independency1}
\caption{Independency}
\label{fig:independency1}
\setfloatalignment{b}
\end{figure}

The graphs look similar but there is a big difference. A proper scientific experiment uses the \emph{design of experiments}-methodology\index{design of experiments}\index{DOE!Design of Experiments}. One of its most important aspects is to make sure that each set of observations of temperature and pressure is made \emph{independently} of previous or future measurements. This is done by \emph{randomisation}\index{randomisation}. Instead of starting with a temperature of 0\degree C and stepswise increasing the temperature with 20\degree C until we reach 180\degree C (the \emph{standard order}\index{standard order}), we will do this in a random way (the \emph{run order}\index{run order}).

<<>>=
run.order <- sample(seq(0, 180, 20))
run.order
@

We start at 80\degree C, then we go to 100\degree C, then to 140\degree C and then we cool to 20\degree C etc. It is obvious that this requires much more time for heating, cooling, reheating etc. than when we use the standard order. However, it is worth it because in this way we eliminate the possible influence of a steady increase in temperature on the measurement (e.g. it might steadily change the volume of the gascontainer due to expansion). In short: we set up our experiment is such a way that we the observations are \emph{independent}\index{independency} of each other (Figure~\ref{fig:independency2}-left). Plotting the measurement of pressure as a function of the moment in time that it was recorded, and assuming it takes 1 hour between two consecutive measurements, we get something like Figure~\ref{fig:independency2}-middle\sidenote{In reality the measurements will probably not be nicely evenly spaced}. 

We created the \emph{run order}\index{run order} by making a random permutation of the temperatures 0\degree C, 20\degree C, ..., 180\degree C. This makes it random, but not independent. Independency is only guaranteed when we do a random pick with \emph{replacement}. This is the second essential component in the design of an experiment: \emph{replication}\index{replication}. A \emph{run order with replacement}\index{run order!with replacement} for a total of 100 experiments can be generated in \textbf{\textsf{R}} and gives Figure~\ref{fig:independency2}-right.

<<>>=
run.order.rep <- data.frame(time = seq(1, 100),
                            temp = sample(seq(0, 180, 20),
                                          size = 100,
                                          replace = TRUE))
@

<<label=independency2, fig=TRUE, include=FALSE, echo=FALSE>>=
p1 <- ggplot(data = ideal) +
        geom_point(aes(x = temp, y = press), col = "red") +
        geom_point(aes(x = temp, y = (press + 0.035)), 
                   col = "black", shape = 1, size = 5) +
        geom_text(aes(x = temp, y = (press + 0.035)), 
                  label = c("5", "4", "7", "8", "1", "2", "9", "3", "6", "10"), size = 3) +
        labs(title = "Experiment\nideal gas law", x = "temperature (°C)", y = "pressure (bar)") +
        scale_y_continuous(limits = c(0.9, 1.9)) +
        JT.theme
ideal$ts <- c(ideal$press[5], ideal$press[6], ideal$press[8], ideal$press[2], ideal$press[1], ideal$press[9], ideal$press[3], ideal$press[4], ideal$press[7], ideal$press[10])
ideal$time <- seq(1,10)
p2 <- ggplot(data = ideal) +
        geom_point(aes(x = time, y = ts), col = "red") +
        geom_point(aes(x = time, y = (ts + 0.035)), 
                   col = "black", shape = 1, size = 5) +
        geom_text(aes(x = time, y = (ts + 0.035)), 
                  label = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"), size = 3) +
        labs(title = "Experiment\nrun order", x = "time (h)", y = "pressure (bar)") +
        scale_y_continuous(limits = c(0.9, 1.9)) +
        JT.theme
p3 <- ggplot(data = run.order.rep) +
        geom_point(aes(x = time, y = temp)) +
        labs(title = "Experiment\nrun order with replacement", x = "time (h)", y = "temperature (°C)") +
        scale_y_continuous(limits = c(-20, 200)) +
        JT.theme
grid.arrange(p1, p2, p3, nrow = 1)
@

\begin{figure}
\includegraphics[width=0.8\textwidth]{PTS-independency2}
\caption{Sequence of observations}
\label{fig:independency2}
\setfloatalignment{b}
\end{figure}

This looks very much like a \emph{stationary} time series and the \emph{Ljung-Box}-test confirms this.

<<>>=
Box.test(run.order.rep$temp, type = "Ljung-Box")
@
\medskip
With the time series of the EU-migrants to the UK this is not possible. We cannot design our measurements in such a way that the first measurement will be taken in June 2010 (Q2 of 2010), the next mea\-surement in December 2011 (Q4 of 2011), the third in December 2009 (Q4 of 2009) etc. Either the future is not accessible, or if we wait, the past cannot be redone. We have not choice but to make our observations starting in Q4 2009, ending in Q2 2012 and this every quarter in its proper turn (Figure~\ref{fig:independency2}-right). Time is an absolute reference point that will only occurs once. Therefore we cannot guarantee that the observations in a time series are \emph{independent}.

<<label=independency3, fig=TRUE, include=FALSE, echo=FALSE>>=
ggplot(data = EUimm) +
      geom_point(aes(x = date, y = number), col = "red") +
      geom_point(aes(x = date, y = (number + 0.035)), 
                  col = "black", shape = 1, size = 5) +
      geom_text(aes(x = date, y = (number + 0.035)), 
                  label = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11"), size = 3) +
      labs(title = "Time series\nEU-immigrants to the UK", x = "date", y = "2009 Q4 = 1.0") +
      scale_y_continuous(limits = c(0.9, 1.9)) +
      JT.theme
@

\begin{figure}
\includegraphics[width=0.8\textwidth]{PTS-independency3}
\caption{Sequence of observations in time series}
\label{fig:independency3}
\setfloatalignment{b}
\end{figure}

For general time series it is prudent to suppose that the measurements are \emph{not independent}\index{not independent}.The uncertainty about independence has serious consequences because many standard statistical methods (e.g. linear regression) can only be applied securely on condition that the observations are independent.

\newpage
For example: Figure~\ref{fig:Amtrakdata} shows the monthly number (in thousands) of people who used the Amtrak railway system between Januari 1991 up to March 2004.

<<label=Amtrakbackground,fig=TRUE,include=FALSE, echo=FALSE>>=
Amtrak.data <- read.csv("Data/Amtrak data.csv", sep=";", stringsAsFactors = FALSE)
# transforming the ''Month" column into a proper date variable
Amtrak.data %>%  mutate(day = myd(paste0(Month, "-01"))) -> Amtrak.data
Amtrak.data$t <- c(1:nrow(Amtrak.data))
datebreaks <- seq(as.Date("1991-01-01"), as.Date("2009-06-01"), by = "2 year")
ggplot(data=Amtrak.data) +
                        geom_line(aes(x=day, y=Ridership), size=0.75, color="white", alpha = 0) +
                        scale_x_date(breaks = datebreaks, labels = date_format("%Y-%m")) +
                        expand_limits(x = as.Date(c("1990-06-01", "2007-06-01"))) +
                        labs(title="Monthly Amtrak ridership data", 
                             x = "Date",
                             caption = "Practical Time Series Forecasting with R - Shmueli and Lichtendahl Jr.") +
                        JT.theme
@

<<label=Amtrakdata,fig=TRUE,include=FALSE, echo=FALSE>>=
Amtrak.data <- read.csv("Data/Amtrak data.csv", sep=";", stringsAsFactors = FALSE)
# transforming the ''Month" column into a proper date variable
Amtrak.data %>%  mutate(day = myd(paste0(Month, "-01"))) -> Amtrak.data
Amtrak.data$t <- c(1:nrow(Amtrak.data))
datebreaks <- seq(as.Date("1991-01-01"), as.Date("2009-06-01"), by = "2 year")
Amtrak.baseplot <- ggplot(data=Amtrak.data) +
                          geom_line(aes(x=day, y=Ridership), size=0.75, color="red") +
                          scale_x_date(breaks = datebreaks, labels = date_format("%Y-%m")) +
                          expand_limits(x = as.Date(c("1990-06-01", "2007-06-01"))) +
                          labs(title="Monthly Amtrak ridership data", 
                               x = "Date",
                               caption = "Practical Time Series Forecasting with R - Shmueli and Lichtendahl Jr.") +
                          JT.theme
Amtrak.baseplot
@

\begin{marginfigure}[-1cm]
\includegraphics[width=1\textwidth]{PTS-Amtrakdata}
\caption{Amtrak: data}
\label{fig:Amtrakdata}
\setfloatalignment{b}
\end{marginfigure}

Our eyes suggest that there are patterns within these data: there is a wave-like motion around a more general curve that first goes down and then starts to rise (Figure~\ref{fig:Amtraktrendseason})

\begin{marginfigure}
\includegraphics[width=1\textwidth]{PTS-Amtraktrendseason}
\caption{Amtrak: pattern}
\label{fig:Amtraktrendseason}
\setfloatalignment{b}
\end{marginfigure}

With this pattern in mind we have a tendency to \emph{extend} this into to future and to generate something like Figure~\ref{fig:Amtrakpred}.

\begin{figure}
\includegraphics[width=1\textwidth]{PTS-Amtrakpred}
\caption{Amtrak: into the future}
\label{fig:Amtrakpred}
\setfloatalignment{b}
\end{figure}

Even when we use \emph{cross-sectional}\index{data!cross-sectional} data\sidenote[][-2cm]{cross-sectional data are collected by observing many subjects at the same point of time. E.g. the exam results of a group of students, the SAT-scores of these students at the end of their secondary education} to try to find a connection between these data (e.g. by linear regression), we implore our students to take care \emph{not} to extend models based on these data \emph{outside} the range into which they were developed. Forecasting, on the contrary, wants to do exactly this.

\newpage
\section{Appendix 1}
\label{sec:appendix1}

A classical physics experiment is investigating the distance covered by a free falling ball in Earth's gravity. The distance traveled in a time $t$ is given by:
\begin{equation}
	y(t)=\frac{g t^{2}}{2} \quad with \quad g=9.81 m/s^{2}
\end{equation}

However, nothing forces us to measure the distance $y$ for $t=0s$, then $t=1 s$, then $t=2s$ etc. We can randomise the experiment and make the first the first measurement at $t=8s$, then at $t=5s$, than $t=1s$ etc. This means that the results will be independent of each other. Essentially the question is: are we constrained to make our observations in a strict sequence.

\printindex

\newpage

\textbf{Thanks} \\
\medskip
R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
\medskip
<<>>=
sessionInfo()
@

\end{document}